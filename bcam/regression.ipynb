{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "The family of algorithms where the targets are continous values (rather than labels/categories)\n",
    "\n",
    "**Index**\n",
    "* [Linear Models](#Linear-models)\n",
    "  * [Ordinary least squares](#Ordinary-least-squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models\n",
    "\n",
    "As seen on [classification](./classification.ipynb/#Logistic-Regression), regression's linear models basically assumes that any target in the data can be approximated by a linear combination of its features.\n",
    "\n",
    "### Ordinary least squares\n",
    "Ordinary least squares seeks to minimize the residual sum of squares between the observed targets in the dataset and the targets predicted by the linear approximation.\n",
    "\n",
    "**!important**\n",
    "Relies on the independence of the features, that is, they are no linearly dependent (no correlation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "d0 = datasets.load_diabetes()\n",
    "# print(d0.DESCR)  # Print the description of the dataset\n",
    "X, y = d0.data, d0.target\n",
    "\n",
    "# Use only one feature, no#2: BMI\n",
    "X = X[:, np.newaxis, 2]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test = X[:-20], X[-20:]\n",
    "y_train, y_test = y[:-20], y[-20:]\n",
    "\n",
    "# Train the model\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make some predictions\n",
    "y_hat = regr.predict(X_test)\n",
    "\n",
    "# Print some outcomes\n",
    "print('Coefficient:                  {}'.format(regr.coef_))\n",
    "print('Mean squared error:           {}'.format(\n",
    "    mean_squared_error(y_test, y_hat).round()))\n",
    "print('Coefficient of determination: {}'.format(\n",
    "    r2_score(y_test, y_hat).round(2)))  # 1.0 is best\n",
    "\n",
    "# Plot the results\n",
    "sns.set()\n",
    "cmap = sns.cubehelix_palette(rot=-.2, as_cmap=True)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_test.ravel(), y=y_test, hue=y_test, ax=ax,)\n",
    "ax.plot(X_test, y_hat)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
