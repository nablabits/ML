{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets, linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# import some data to play with\n",
    "IRIS = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mesh(X, h=.01):\n",
    "    \"\"\"Create a meshgrid object with the input space dimensions.\"\"\"\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return (xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "* [About classification algorithms](#About-classification-algorithms)\n",
    "* [Train and test subsamples](#Train-and-test-subsamples)\n",
    "* [K nearest neigbors](#KNN)\n",
    "* [Decision Trees](#Decision-Trees)\n",
    "    * [Splitting the dataset](#Splitting-the-dataset)\n",
    "    * [Avoiding overfitting](#Avoiding-overfitting)\n",
    "* [Logistic regression](#Logistic-Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About classification algorithms\n",
    "Some classification algorithms can only distinguish between two classes, how can we use them in multi class problems? There are two approaches to this:\n",
    "    \n",
    "* **One vs one:** is the approach where we evaluate the classes in pairs. Say we have three classes, A, B and C. The OVO ensemble will be composed of 3 (= 3 * (3 - 1) / 2) binary classifiers. The first will discriminante A from B, the second A from C, and the third B from C. At prediction stage, the class that got the highest number of \"+1\" predictions is our winner. Notice that this is a $O(n^2)$ problem\n",
    "      \n",
    "* **One vs rest:** (aka one-vs-all)is the strategy that involves training one classifier (estimator) for class and then taking the one which gives the highest confidence.\n",
    "\n",
    "[wiki](https://en.wikipedia.org/wiki/Multiclass_classification#Transformation_to_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test subsamples\n",
    "In general we should split the data given in two parts: one for training and the other for testing. Usually the testing slice is 1/3 of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "K nearest neigbors is a *lazy* algorithm which does not learn and makes computations in classification time, that is, find a predefined number of training samples (k) closest in distance to the new point, and predict the label from these.\n",
    "\n",
    "Notice that KNN takes by default the k closest samples regardless how far they are, to mitigate this effect a weight parameter can be added.\n",
    "\n",
    "KNN can also be applied to time series but they're pretty much regression problems we'll see them in due time. \n",
    "\n",
    "**Key features of KNN:**\n",
    "* Easy to understand and implement.\n",
    "* Computationally efficient in general (with small datasets).\n",
    "* Defining similarities.\n",
    "* The first thing that should be tried when approaching a ML problem.\n",
    "* They suffer especially the [curse of dimensionality](./../Glossary.ipynb/#C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PART #1, Load an preprocess the data #####\n",
    "\n",
    "# we only take the first two features in the dataset\n",
    "X = IRIS.data[:, :2]\n",
    "cols = IRIS['feature_names'][:2]\n",
    "y = IRIS.target\n",
    "\n",
    "\n",
    "##### PART 2, create the model #####\n",
    "\n",
    "# Number of neighbors and weight\n",
    "k, w = 30, 'distance'\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors=k, weights=w)\n",
    "clf.fit(X, y)\n",
    "\n",
    "\n",
    "##### PART #3, plot the outcome ####\n",
    "\n",
    "# We are about to create a mesh of points that will represent a bunch of predictions \n",
    "xx, yy = mesh(X)\n",
    "    \n",
    "# Once created the mesh, drop all the points into the model and predict the values for them\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Plot the prediction areas (background)\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "Z = Z.reshape(xx.shape)  # reshape to match the grid, same as yy.shape\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points (real points)\n",
    "cmap_bold =  ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n",
    "plt.xlabel(cols[0])\n",
    "plt.ylabel(cols[1])\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = %i)\"% (k))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "[Nice visualization](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/) \n",
    "\n",
    "Decision trees divide the space into high dimensional rectangles. They are simple to understand and interpret (white box model), but they tend to overfit the data. However, they are useful in other ML techniques like bagging or random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take the features in pairs (Uncomment to see other pairs)\n",
    "pair = [0,1]\n",
    "#pair = [1,2] \n",
    "#pair = [2,3] \n",
    "X = IRIS.data[:, pair]\n",
    "y = IRIS.target\n",
    "\n",
    "# Train\n",
    "clf = DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "# Display the score (in the same training set, notice)\n",
    "print('score was: {}'.format(clf.score(X, y)))\n",
    "\n",
    "# Again, create a mesh of points that will represent a bunch of predictions\n",
    "xx, yy = mesh(X)\n",
    "\n",
    "# Once created the mesh, drop all the points into the model and predict the values for them\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "plt.xlabel(IRIS.feature_names[pair[0]])\n",
    "plt.ylabel(IRIS.feature_names[pair[1]])\n",
    "\n",
    "# Plot the training points\n",
    "plot_colors, n_classes = 'ryb', 3\n",
    "for i, color in zip(range(n_classes), plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=IRIS.target_names[i], edgecolor='black', s=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n",
    "Let's split the dataset into training and testing subsamples so we can check how effective is our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y)\n",
    "\n",
    "# Train\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Display the score\n",
    "print('score was: {}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "# Plot the decision boundary\n",
    "xx, yy = mesh(X)\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "plt.xlabel(IRIS.feature_names[pair[0]])\n",
    "plt.ylabel(IRIS.feature_names[pair[1]])\n",
    "\n",
    "# Plot the training points\n",
    "for i, color in zip(range(n_classes), plot_colors):\n",
    "    idx = np.where(y_test == i)\n",
    "    plt.scatter(X_test[idx, 0], X_test[idx, 1], c=color, label=IRIS.target_names[i], edgecolor='black', s=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding overfitting\n",
    "As we can see, decision trees tend to overfit to the trining data, there are two ways to mitigate this:\n",
    "* Bagging **(B**ootstrap **agg**regat**ing**) [[wiki]](https://en.wikipedia.org/wiki/Bootstrap_aggregating)\n",
    "* Random Forests [[wiki]](https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "**A: Bagging:** take several random subsets of the data, train them independently and finally aggregate the resutls and vote the best one.\n",
    "\n",
    "**B: Random Forests:** Instead of taking random subsets of the data, we take random subsets of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an artificial dataset\n",
    "X, y = datasets.make_classification(n_samples=10000, n_features=6,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "\n",
    "# Now split the train and the test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y)\n",
    "\n",
    "# Train the model\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=100, max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)  \n",
    "\n",
    "# Display the score\n",
    "print('score was: {}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "print(clf.predict([[0, 0, 0, 0, 0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "[Regression](https://en.wikipedia.org/wiki/Regression_analysis) is a wide topic in maths and machine learning that tries to estimate an outcome (target) given several independent variables called predictors or features so we can forecast a future output.\n",
    "\n",
    "The basic idea of regression is the following:\n",
    "\n",
    "\n",
    "$$\\hat{y}(\\mathbf{w},\\mathbf{x})=w_0+\\mathbf{w_1 x_1}+...+\\mathbf{w_p y_p}$$\n",
    "\n",
    "\n",
    "We'll try to predict a $\\hat{y}$ by assigning a coeficient ($\\mathbf{w}$, weight) to each component (feature) of the vector $\\mathbf{X}$ we input and an intercept point (constant term) $w_0$.\n",
    "\n",
    "That is: we assume that **every target in the data can be approximated by a linear combination of its features.**\n",
    "\n",
    "In the case of logistic regression, we can plug this line as an argument of the logistic function to get a probability for a certain sample $X$ to be classified as 0 or 1. Odds under 2:2 will be classified as $0$ and $1$ otherwise.   \n",
    "\n",
    "#### **Example #1) logistic regresion with random samples**\n",
    "An adaptation from a [[scikit-learn]](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html#sphx-glr-auto-examples-linear-model-plot-logistic-py) example. \n",
    "\n",
    "* **Black dots:** a random sample where values over 0 yield $y=1$ (mostly but not always) and $y=0$ otherwise.\n",
    "\n",
    "* **Blue line:** a logistic model that predict a certain value. Every prediction for $X>0.222$ ($-w_0/w_1$) will be classified as $1$ otherwise $0$.\n",
    "\n",
    "* **Red curve:** the probabilty that the prediction will be 1. Notice that the point where probability is $0.5$ is precisely $-w_0/w_1$\n",
    "\n",
    "There is also a [Desmos graph](https://www.desmos.com/calculator/binjtdtjry) to get a feeling of how those coefficients affect to the final logistic curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of 300 random samples cetered at 0 \n",
    "n_samples = 300\n",
    "np.random.seed(0)\n",
    "X = np.random.normal(size=n_samples)\n",
    "\n",
    "# Now, add value 1 to samples over 0 and 0 to samples under 0\n",
    "y = (X > 0).astype(np.float).ravel()\n",
    "\n",
    "# Add some noise\n",
    "X[X > 0] *= 4  # strectch out values over 0\n",
    "\n",
    "# Be sure to find samples under 0 with y=1 and vice versa\n",
    "X += .3 * np.random.normal(size=n_samples)  \n",
    "\n",
    "# Finally, make it a col vector\n",
    "X = X[:, np.newaxis]\n",
    "\n",
    "# Plot the point distribution\n",
    "plt.figure(figsize=(17, 5))\n",
    "plt.scatter(X, y, color='black')\n",
    "\n",
    "# Instantiate the classifier\n",
    "clf = linear_model.LogisticRegression(C=1e7, solver='lbfgs')\n",
    "clf.fit(X, y)\n",
    "w0, w1 = clf.intercept_, clf.coef_\n",
    "\n",
    "def log(x):\n",
    "    \"\"\"Get the probability using a logistic function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Create a bunch of test samples and predict the values and the odds for them.\n",
    "X_test = np.linspace(-1, 2, n_samples)\n",
    "y_hat = clf.predict(X_test[:, np.newaxis])\n",
    "odds = log(w1 * X_test + w0).ravel()\n",
    "\n",
    "# Plot the outcomes\n",
    "plt.plot(X_test, odds, color='red', linewidth=3, label='Probability')\n",
    "plt.plot(X_test, y_hat, color='blue', linewidth=3, label='Prediction (y_hat)')\n",
    "\n",
    "# Finally, add some more details to the plot\n",
    "plt.axhline(.5, color='.5') # 2:2 odds\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('X')\n",
    "plt.xlim(-1, 2)\n",
    "plt.legend( loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Example #2) logistic regresion with iris dataset**\n",
    "An adaptation from a [[scikit-learn]](https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html#sphx-glr-auto-examples-linear-model-plot-iris-logistic-py) example. \n",
    "\n",
    "In this example, we'll tray to set a linear boundary between the points so blue ones are in blue areas, yellows in yellow and browns in brown.\n",
    "\n",
    "We can imagine this boundaries as different heights (Z). We can see that blue ones are quite accurate classified whereas the main problem is addressed at brown ones.\n",
    "\n",
    "The second graph shows how some pair of features are more suitable to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, 2:4]  # we only take the first two features.\n",
    "Y = iris.target\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg = linear_model.LogisticRegression(\n",
    "    C=1, solver='lbfgs', multi_class='multinomial')\n",
    "logreg.fit(X, Y)\n",
    "\n",
    "# Plot the decision boundary.\n",
    "xx, yy = mesh(X)\n",
    "\n",
    "# Drop all the points into the model and predict the values for them\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(7,7))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-comparison between pairs od features.\n",
    "\n",
    "# Let's take features by couples to compare them\n",
    "X = (IRIS.data[:, :2], IRIS.data[:, 1:3], IRIS.data[:, 2:4])\n",
    "Y = IRIS.target\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg = linear_model.LogisticRegression(\n",
    "    C=1e5, solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "# Instantiate the plot\n",
    "_, axs = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "for n, pair in enumerate(X):\n",
    "    # Fit the model\n",
    "    clf = logreg.fit(pair, Y)\n",
    "    \n",
    "    # Get the boundaries\n",
    "    xx, yy = mesh(pair)\n",
    "    \n",
    "    # Make a bunch of predictions\n",
    "    y_hat = (clf.predict(np.c_[xx.ravel(), yy.ravel()]))\n",
    "    Z = (y_hat.reshape(xx.shape))\n",
    "    \n",
    "    # Plot the classification boundaries\n",
    "    axs[n].pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    \n",
    "    # And the training points\n",
    "    axs[n].scatter(pair[:, 0], pair[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "    \n",
    "    # Now set the limits, labels and remove the ticks\n",
    "    axs[n].set_xlim(xx.min(), xx.max())\n",
    "    axs[n].set_ylim(yy.min(), yy.max())\n",
    "    axs[n].set_xlabel(IRIS.feature_names[n])\n",
    "    axs[n].set_ylabel(IRIS.feature_names[n + 1])\n",
    "    axs[n].set_xticks(()), axs[n].set_yticks(())\n",
    "    \n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
