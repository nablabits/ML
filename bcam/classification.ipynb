{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "* [About classification algorithms](#About-classification-algorithms)\n",
    "* [Train and test subsamples](#Train-and-test-subsamples)\n",
    "* [K nearest neigbors](#KNN)\n",
    "* [Decision Trees](#Decision-Trees)\n",
    "    * [Splitting the dataset](#Splitting-the-dataset)\n",
    "    * [Avoiding overfitting](#Avoiding-overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About classification algorithms\n",
    "Some classification algorithms can only distinguish between two classes, how can we use them in multi class problems? There are two approaches to this:\n",
    "    \n",
    "* **One vs one:** is the approach where we evaluate the classes in pairs. Say we have three classes, A, B and C. The OVO ensemble will be composed of 3 (= 3 * (3 - 1) / 2) binary classifiers. The first will discriminante A from B, the second A from C, and the third B from C. At prediction stage, the class that got the highest number of \"+1\" predictions is our winner. Notice that this is a $O(n^2)$ problem\n",
    "      \n",
    "* **One vs rest:** (aka one-vs-all)is the strategy that involves training one classifier (estimator) for class and then taking the one which gives the highest confidence.\n",
    "\n",
    "[wiki](https://en.wikipedia.org/wiki/Multiclass_classification#Transformation_to_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test subsamples\n",
    "In general we should split the data given in two parts: one for training and the other for testing. Usually the testing slice is 1/3 of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "K nearest neigbors is a *lazy* algorithm which does not learn and makes computations in classification time, that is, find a predefined number of training samples (k) closest in distance to the new point, and predict the label from these.\n",
    "\n",
    "Notice that KNN takes by default the k closest samples regardless how far they are, to mitigate this effect a weight parameter can be added.\n",
    "\n",
    "KNN can also be applied to time series but they're pretty much regression problems we'll see them in due time. \n",
    "\n",
    "**Key features of KNN:**\n",
    "* Easy to understand and implement.\n",
    "* Computationally efficient in general (with small datasets).\n",
    "* Defining similarities.\n",
    "* The first thing that should be tried when approaching a ML problem.\n",
    "* They suffer especially the [curse of dimensionality](./../Glossary.ipynb/#C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PART #1, Load an preprocess the data #####\n",
    "\n",
    "# we only take the first two features in the dataset\n",
    "X = iris.data[:, :2]\n",
    "cols = iris['feature_names'][:2]\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "##### PART 2, create the model #####\n",
    "\n",
    "# Number of neighbors and weight\n",
    "k, w = 30, 'distance'\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors=k, weights=w)\n",
    "clf.fit(X, y)\n",
    "\n",
    "\n",
    "##### PART #3, plot the outcome ####\n",
    "\n",
    "# We are about to create a mesh of points that will represent a bunch of predictions \n",
    "h = .01  # step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Once created the mesh, drop all the points into the model and predict the values for them\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Plot the prediction areas (background)\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "Z = Z.reshape(xx.shape)  # reshape to match the grid, same as yy.shape\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points (real points)\n",
    "cmap_bold =  ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n",
    "plt.xlabel(cols[0])\n",
    "plt.ylabel(cols[1])\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = %i)\"% (k))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "[Nice visualization](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/) \n",
    "\n",
    "Decision trees divide the space into high dimensional rectangles. They are simple to understand and interpret (white box model), but they tend to overfit the data. However, they are useful in other ML techniques like bagging or random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take the features in pairs (Uncomment to see other pairs)\n",
    "pair = [0,1]\n",
    "#pair = [1,2] \n",
    "#pair = [2,3] \n",
    "X = iris.data[:, pair]\n",
    "y = iris.target\n",
    "\n",
    "# Train\n",
    "clf = DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "# Display the score (in the same training set, notice)\n",
    "print('score was: {}'.format(clf.score(X, y)))\n",
    "\n",
    "# Again, create a mesh of points that will represent a bunch of predictions\n",
    "plot_step = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),np.arange(y_min, y_max, plot_step))\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "\n",
    "# Once created the mesh, drop all the points into the model and predict the values for them\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "plt.xlabel(iris.feature_names[pair[0]])\n",
    "plt.ylabel(iris.feature_names[pair[1]])\n",
    "\n",
    "# Plot the training points\n",
    "plot_colors, n_classes = 'ryb', 3\n",
    "for i, color in zip(range(n_classes), plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i], edgecolor='black', s=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n",
    "Let's split the dataset into training and testing subsamples so we can check how effective is our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y)\n",
    "\n",
    "# Train\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Display the score\n",
    "print('score was: {}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "# Plot the decision boundary\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),np.arange(y_min, y_max, plot_step))\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "plt.xlabel(iris.feature_names[pair[0]])\n",
    "plt.ylabel(iris.feature_names[pair[1]])\n",
    "\n",
    "# Plot the training points\n",
    "for i, color in zip(range(n_classes), plot_colors):\n",
    "    idx = np.where(y_test == i)\n",
    "    plt.scatter(X_test[idx, 0], X_test[idx, 1], c=color, label=iris.target_names[i], edgecolor='black', s=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding overfitting\n",
    "As we can see, decision trees tend to overfit to the trining data, there are two ways to mitigate this:\n",
    "* Bagging **(B**ootstrap **agg**regat**ing**) [[wiki]](https://en.wikipedia.org/wiki/Bootstrap_aggregating)\n",
    "* Random Forests [[wiki]](https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "**A: Bagging:** take several random subsets of the data, train them independently and finally aggregate the resutls and vote the best one.\n",
    "\n",
    "**B: Random Forests:** Instead of taking random subsets of the data, we take random subsets of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an artificial dataset\n",
    "X, y = datasets.make_classification(n_samples=10000, n_features=6,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "\n",
    "# Now split the train and the test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y)\n",
    "\n",
    "# Train the model\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=100, max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)  \n",
    "\n",
    "# Display the score\n",
    "print('score was: {}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "print(clf.predict([[0, 0, 0, 0, 0, 0]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
