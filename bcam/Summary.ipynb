{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary \n",
    "\n",
    "This is the summary of the crash course took at [BCAM](http://www.bcamath.org/en/) on 2019 40th week\n",
    "\n",
    "\n",
    "**Index**  \n",
    "* [Introduction](#Introduction)  \n",
    "* [Preprocessing](#Preprocessing)\n",
    "    * [Outliers](#Section-A%3A-Outliers)\n",
    "    * [NaN values](#Section-B%3A-NaN-values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "There are three steps when it comes to ML:\n",
    " * Preprocess: is about ~80% of the time, we prepare and clean the data we are about to work with. In this stage we also select the features we'll work with.\n",
    " * Process: Where the learning takes place.\n",
    " * Evaluating: measuring the quality of our predictions.\n",
    " \n",
    "We can come across with two kind of data: the one that shows both input variables and the output for them (categorical or numerical), and the one that only shows the inputs and we must infer the outputs. The learning that deals with first one is called **supervised learning** [wiki](https://en.wikipedia.org/wiki/Supervised_learning), whereas the one that deals with the latter is **non-supervised learning.** [mathworks](https://www.mathworks.com/discovery/unsupervised-learning.html)\n",
    "\n",
    "Supervised learning works mainly in classification (the outputs are categories) & regression (the outputs are numbers).\n",
    "\n",
    "Unsupervised learning deals with clustering, aka, group similar elements together.\n",
    "\n",
    "We were also playing with the iris dataset in scikit learn.\n",
    "Quite interesting the pairplot method of seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting familiar with the data\n",
    "The following cells are a sample to get familiar with the data we'll work with. Also remember that expert knowledge is quite relevant, so always try to stay in contact with the data providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (a set of numpy arrays)\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Convert to a DF\n",
    "iris = pd.DataFrame(data= np.c_[iris['data'], iris['target']],columns= iris['feature_names'] + ['target'])\n",
    "iris['target'] = iris['target'].astype(int)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a quick summarizing view about the data\n",
    "iris.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pair plots for each column in the dataset to find out relationships\n",
    "sns.pairplot(iris, vars=iris.columns[:-1], hue='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice several things here:\n",
    "* the target 0 (Setosa) is fairly different from the others. Their petal dimesions are significantly smaller than the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The violin plot in seaborn is mix between a box plot and a kernel density function\n",
    "fig, ax = pyplot.subplots(figsize =(9, 7)) \n",
    "sns.violinplot( ax = ax, y = iris[\"petal width (cm)\"], x = iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "**Preprocessing** is the stage in a machine learning pipeline where we clean and make sense of the data we have. It takes the **80%** of the time spent in a ML project. THe bigger the data the more important its preprocessing process.\n",
    "\n",
    "**Expert knowledge** is always relevant. It provides the most important features, the ranges for variables, helps identifying redundancies and set bounds to the modeling choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section A: Outliers\n",
    "Outliers are often bad data points [1](https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm). \n",
    "They can be detected collectivelly (using Mahalanobis distance) or individually, that is, feature by feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df=pd.read_csv('iris_data.csv')\n",
    "\n",
    "# Get a copy to work with\n",
    "df_A = df.copy()\n",
    "\n",
    "# let's look for outliers into the whole data (uncomment to see)\n",
    "ax = sns.boxplot(data=df_A[df_A.columns[:-1]], orient=\"h\", palette=\"Set2\", linewidth=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we see that petal_width has some of them, let's plot it\n",
    "ax2 = sns.boxplot(data=df_A['sepal_width'], orient=\"h\", color=sns.color_palette(\"Set2\")[1], linewidth=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we're going to get rid of them\n",
    "# First, define interquartile range\n",
    "df_A_stats=df_A.describe()\n",
    "IQR = df_A_stats[\"sepal_width\"][\"75%\"] - df_A_stats[\"sepal_width\"][\"25%\"]\n",
    "\n",
    "# And the whiskers\n",
    "whiskers = [df_A_stats[\"sepal_width\"][\"25%\"]-(1.5*IQR),\n",
    "            df_A_stats[\"sepal_width\"][\"75%\"]+(1.5*IQR)]\n",
    "\n",
    "# Get the outliers\n",
    "outliers=df_A[\n",
    "    (df_A['sepal_width'] > whiskers[1]) |\n",
    "    (df_A['sepal_width'] < whiskers[0])\n",
    "]\n",
    "\n",
    "# Now, drop'em all\n",
    "data1_outliers=df_A.drop(index=outliers.index)\n",
    "assert(data1_outliers.shape[0] == 146)  # Four indices were dropped as expected\n",
    "\n",
    "# Finally plot the graph to ensure everything was fine\n",
    "ax2 = sns.boxplot(y=\"sepal_width\", data=data1_outliers, orient=\"h\", color=sns.color_palette(\"Set2\")[1], linewidth=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B: NaN values\n",
    "\n",
    "When coming across with NaNs, two approaches can be taken:\n",
    "* If they are few of the total dataset and the dataset is large, we can remove them.\n",
    "* Otherwise they can be guessed by especific algorithms.\n",
    "\n",
    "### Getting rid of NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get a copy of the original dataframe\n",
    "df_nan = df.copy()\n",
    "\n",
    "# Create some nans\n",
    "df_nan.iloc[1, 0] = np.nan\n",
    "df_nan.iloc[5:7, 1] = np.nan\n",
    "df_nan.iloc[7, 3] = np.nan\n",
    "\n",
    "# Get rid of nan values and reindex the df\n",
    "df_not_nan = df_nan.dropna().reset_index()\n",
    "\n",
    "# There should be no NaNs\n",
    "assert(((df_not_nan != np.isnan).any()).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIlling in the NaNs with the mean value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an  array of the values\n",
    "X_nan = df_nan.iloc[:, :-1].values\n",
    "\n",
    "# Now instantiate the imputer from sklearn\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# Calculate for X_nan and transform the original values\n",
    "X_imputed = imp.fit(X_nan).transform(X_nan)\n",
    "\n",
    "# Check that the first value imputed actually has the mean value in the previous dataset\n",
    "assert(df_nan.iloc[:,0].mean() == X_imputed[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section C: Standarization & normalization\n",
    "*From the slides*\n",
    "* When  computing  distances  between  pairs  of  samples,  the  scales  of  the different features is very relevant.\n",
    "\n",
    "* Moreover, we cannot obviate the curse of dimensionality effect, i.e. in high-dimensional spaces all data is sparse. In simple words, all distances become huge.\n",
    "\n",
    "* Therefore, if  the  algorithm  we  plan  to  apply  after  preprocessing  implies distances and/or we are in a high-dimensional problem, we should transform all the features to a similar scale.\n",
    "\n",
    "***\n",
    "**Standarization:** Standardization (or Z-score normalization) is the process of rescaling the features so that they’ll have the properties of a Gaussian distribution with μ=0 and σ=1 where μ is the mean and σ is the standard deviation from the mean\n",
    "\n",
    "**Normalization:** the process of scaling individual samples to have unit norm, that is, they will be in the range {0, 1} or {-1, 1}. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples. [s](https://scikit-learn.org/stable/modules/preprocessing.html#normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new copy of the data\n",
    "data2=df.copy()\n",
    " \n",
    "# Split into input space and output space\n",
    "X2 = data2[['sepal_length','sepal_width', 'petal_length', 'petal_width']]\n",
    "y2 = data2['species']\n",
    "\n",
    "# Instantiate the standard scaler (standarization)\n",
    "sc = StandardScaler()\n",
    "X2_standarized = sc.fit_transform(X2)\n",
    "\n",
    "# Ensure that the values are --almost- 0 & 1 respectivelly\n",
    "assert((X2_standarized.mean(axis=0) < 1e-10).all())\n",
    "assert((X2_standarized.std(axis=0) < 1 + 1e-10).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the MinMaxScaler & apply\n",
    "sc = MinMaxScaler()\n",
    "X2_normalized = sc.fit_transform(X2)\n",
    "\n",
    "# Ensure max=1 & min=0\n",
    "assert((X2_normalized.min(axis=0) == 0).all())\n",
    "assert((X2_normalized.max(axis=0) == 1).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section D: discretization\n",
    "\n",
    "*From the slides*\n",
    "* The goal of discretization is reducing the number of values of a continuous attribute by grouping them into intervals (bins). The new values are the bins.\n",
    "\n",
    "* Some methods require discrete attributes (some naïve Bayes and Bayesian networks methods, etc.).\n",
    "\n",
    "* Sometimes,  the results  are better  after discretization. Some methods do it implicitly (e.g. decision trees).\n",
    "\n",
    "* In  general,  the  computational  cost  of  algorithms  with  discrete  attributes  is lower than their continuous versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new copy of the data to play with\n",
    "data3=df.copy()\n",
    "\n",
    "# Select the features we'll work with\n",
    "X3 = data3[['sepal_length','sepal_width', 'petal_length', 'petal_width']]\n",
    "\n",
    "# Instantiate the discretizers: equal width & equal frequency\n",
    "enc1 = KBinsDiscretizer(n_bins=[3, 4, 2, 2], encode='ordinal', strategy='uniform')\n",
    "enc2 = KBinsDiscretizer(n_bins=[3, 4, 2, 2], encode='ordinal', strategy='quantile')\n",
    "\n",
    "# Calcualte the bins and transform them\n",
    "X_binned_EW = enc1.fit(X3).transform(X3)\n",
    "X_binned_EF = enc2.fit(X3).transform(X3)\n",
    "\n",
    "# Now plot them to see the differences\n",
    "fig =plt.figure(figsize=(20, 20))\n",
    "\n",
    "plt.subplot(421)\n",
    "plt.hist(X_binned_EW[:,0])\n",
    "\n",
    "plt.title('Equal-Width -> sepal_length',fontsize=20)\n",
    "\n",
    "plt.subplot(422)\n",
    "plt.hist(X_binned_EF[:,0])\n",
    "plt.title('Equal-Frequency -> sepal_length',fontsize=20)\n",
    "\n",
    "plt.subplot(423)\n",
    "plt.hist(X_binned_EW[:,1])\n",
    "plt.title('Equal-Width -> sepal_width',fontsize=20)\n",
    "\n",
    "plt.subplot(424)\n",
    "plt.hist(X_binned_EF[:,1])\n",
    "plt.title('Equal-Frequency -> sepal_width',fontsize=20)\n",
    "\n",
    "plt.subplot(425)\n",
    "plt.hist(X_binned_EW[:,2])\n",
    "plt.title('Equal-Width -> petal_length',fontsize=20)\n",
    "\n",
    "plt.subplot(426)\n",
    "plt.hist(X_binned_EF[:,2])\n",
    "plt.title('Equal-Frequency -> petal_length',fontsize=20)\n",
    "\n",
    "plt.subplot(427)\n",
    "plt.hist(X_binned_EW[:,3])\n",
    "plt.title('Equal-Width -> petal_width',fontsize=20)\n",
    "\n",
    "plt.subplot(428)\n",
    "plt.hist(X_binned_EF[:,3])\n",
    "plt.title('Equal-Frequency -> petal_width',fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
