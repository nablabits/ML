{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')  \n",
    "testdf = pd.read_csv('test.csv')  # the data without class for submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian age inference\n",
    "\n",
    "This approach has two main stages:\n",
    "* **Age Inference:** using all the available data, estimate unknown ages' bins by making use of bayes' rule over Titles.\n",
    "* **Surviving inference:** once filled all NaN ages, apply the JeffD approach so they can be comparable.\n",
    "\n",
    "---\n",
    "[Place here the uploads]\n",
    "\n",
    "\n",
    "### 1st attempt\n",
    "`RandomForestClassifier(\n",
    "    max_depth=10, max_features='sqrt', min_samples_leaf=5, n_estimators=9)`  \n",
    "\n",
    "Mean accuracy: 0.829450686641698  \n",
    "No upload\n",
    "\n",
    "---\n",
    "\n",
    "### 2nd attempt\n",
    "`RandomForestClassifier(\n",
    "    criterion='entropy', max_depth=10, max_features='log2',\n",
    "    min_samples_split=5, n_estimators=9)`\n",
    "    \n",
    "Mean accuracy (10 fold): 0.8305617977528088  \n",
    "After these trainings, the submission scored **.76076**\n",
    "\n",
    "---\n",
    "\n",
    "### 3rd attempt\n",
    "\n",
    "On this time I've reduced the `max_depth` because of an overfitting intuition, options to `[2, 3, 5]`:  \n",
    "`RandomForestClassifier(\n",
    "    criterion='entropy', max_depth=5, max_features='log2',\n",
    "    min_samples_leaf=8, min_samples_split=5, n_estimators=4)`  \n",
    "\n",
    "This upload scored the best outcome to date **.77751**\n",
    "\n",
    "---\n",
    "\n",
    "### 4th attempt\n",
    "\n",
    "Finally, I opted for give it one final go, just in case\n",
    "\n",
    "The overfittig intuition gains weight. Also, it seems that when running fresh the notebook (on this attempt I restarted the kernel) the hyperparameters are less.    \n",
    "`RandomForestClassifier(max_depth=5, max_features='log2', n_estimators=6)`  \n",
    "\n",
    "Mean accuracy was set to 0.8193133583021224\n",
    "\n",
    "It scored better than previous: **.77990**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling up unknown ages stage\n",
    "\n",
    "- Join test and train datasets since we are going to use all the available features \n",
    "- Get rid of unknown ages\n",
    "- Clean in a JeffD way.\n",
    "- Train/test split the data so we can check the accuracy of the predictor\n",
    "- Build a linear model using a Bayesian approach that predicts the ages\n",
    "- Measure the accuracy of age inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "We'll create reusable functions so we can reuse them again for the survival stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_bins(df):\n",
    "    \"\"\"Classify ages in logical bins.\"\"\"\n",
    "    df['Age'] = df.Age.fillna(-.5)\n",
    "    bins = [-1, 0, 5, 12, 18, 25, 35, 60, 120, ]\n",
    "    labels = [\n",
    "        'unknown', 'baby', 'child', 'teenager', 'student',\n",
    "        'young adult', 'adult', 'senior', ]\n",
    "    df.loc[:, 'Age'] = pd.cut(df.Age, bins, labels=labels)\n",
    "    return df\n",
    "\n",
    "\n",
    "def cabins(df):\n",
    "    \"\"\"Just keep the initial letter from the cabin number.\"\"\"\n",
    "    df.loc[:, 'Cabin'] = df.Cabin.fillna('N')\n",
    "    df.loc[:, 'Cabin'] = df.Cabin.apply(lambda x: x[0])\n",
    "    return df\n",
    "\n",
    "\n",
    "def fares(df):\n",
    "    \"\"\"Make the fares categorical.\"\"\"\n",
    "    df.loc[:, 'Fare'] = df.Fare.fillna(-.5)\n",
    "    bins = [-1, 0, 8, 14, 31, 520, ]\n",
    "    labels = ['unknown', '1st', '2nd', '3rd', '4th']\n",
    "    df.loc[:, 'Fare'] = pd.cut(df.Fare, bins, labels=labels)\n",
    "    return df\n",
    "\n",
    "\n",
    "def names(df):\n",
    "    \"\"\"Normalize names.\"\"\"\n",
    "    d1 = df.Name.apply(lambda x: x.split(',')[1].split('.')[0])\n",
    "    df['Title'] = d1.str.replace(' ', '')\n",
    "\n",
    "    # A couple of irregular ones\n",
    "    d1 = df[df.Title.str.contains('Jonkheer')]\n",
    "    d2 = df[df.Title.str.contains('Countess')]\n",
    "    df.loc[d1.index, 'Title'] = 'Mr'\n",
    "    df.loc[d2.index, 'Title'] = 'Mrs'  # In her Age group are majority\n",
    "    \n",
    "    # Let's assume that MR == Master\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_useless_cols(df):\n",
    "    \"\"\"Get rid of useless columns\"\"\"\n",
    "    return df.drop(\n",
    "        columns=['Ticket', 'Embarked', 'Name', 'PassengerId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize people per title\n",
    "Once defined the methods, we'll proceed to prepare the data to apply bayes' theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join train and test original datasets\n",
    "# We'll use all the available known ages\n",
    "d0 = df.drop(columns='Survived').copy()\n",
    "d0 = pd.concat((d0, testdf))\n",
    "\n",
    "# get rid of nan ages\n",
    "nan_ages = d0.Age.isna()\n",
    "d0 = d0[~nan_ages]\n",
    "\n",
    "# now transform the data using the methods\n",
    "d0 = age_bins(d0)\n",
    "assert (d0.Age == 'unknown').sum() == 0  # Ensure there are no unknown ages\n",
    "d0 = cabins(d0)\n",
    "d0 = fares(d0)\n",
    "d0 = names(d0)\n",
    "d0 = drop_useless_cols(d0)\n",
    "\n",
    "\n",
    "# count people within each group and title\n",
    "count_people = d0.pivot_table(\n",
    "    index='Title', columns='Age', values='Pclass', aggfunc='count').fillna(0)\n",
    "count_people.columns = count_people.columns.astype(str)\n",
    "\n",
    "# Get a df with the totals for visualization purposes\n",
    "visual_df = count_people.copy()\n",
    "visual_df['total'] = visual_df.sum(axis=1)\n",
    "visual_df.loc['total'] = visual_df.sum()\n",
    "visual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' rule\n",
    "Update the probability of having certain age given a title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "passage = count_people.sum().sum()\n",
    "\n",
    "# Ages prior\n",
    "priors = count_people.sum() / passage\n",
    "\n",
    "# Title likelihoods\n",
    "likelihood = count_people.T / count_people.sum(axis=1)\n",
    "\n",
    "# Get the posterior\n",
    "likelihood.iloc[likelihood == 0] = np.nan\n",
    "posterior = priors * likelihood.T \n",
    "\n",
    "# Since we're not considering all the evidence in the model when calculating\n",
    "# Bayes, we divide by the sum so all the titles' probabilities add up to one. \n",
    "posterior = (posterior.T / posterior.sum(axis=1))\n",
    "posterior.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Col* adult has the same posterior as *Don* since there's only one *Col* in all the passage and therefore its likelihood is 1 (understand 100%). This means that their priors remain intact in the posterior. And that makes sense, you're not incorporating any new information to what you knew \n",
    "\n",
    "Conversely, *Dr* has the likelihood distributed among *student*, *young adult* and *adult* (14% +14% + 72% = 100%) and so it's reflected in its posterior (13%, 14% & 73%). Notice how different passage sizes between *student* & *young adult* (250 vs 281) have slightly effect on the posterior despite of the fact their likelihoods were the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot DR to see differences\n",
    "sns.set()\n",
    "x = posterior.index\n",
    "fig, ax = plt.subplots(3, 2, figsize=(15, 9))\n",
    "sns.barplot(x=x, y=posterior['Dr'], ax=ax[0, 0], alpha=.5)\n",
    "sns.barplot(x=x, y=posterior['Master'], ax=ax[0, 1])\n",
    "sns.barplot(x=x, y=posterior['Miss'], ax=ax[1, 0]);\n",
    "sns.barplot(x=x, y=posterior['Mr'], ax=ax[1, 1]);\n",
    "sns.barplot(x=x, y=posterior['Mrs'], ax=ax[2, 0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify unknown ages\n",
    "Once known the classification rule for unknown ages let's fill in the age predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get the classification Series\n",
    "clf = posterior.T.idxmax(axis=1)\n",
    "clf.name = 'clf'\n",
    "\n",
    "def classify_ages(df, clf=clf):\n",
    "    \"\"\"\n",
    "    Classify unknown ages. \n",
    "    First select the unknown ones' titles. Then, merge the titles with their\n",
    "    correspondent age. Finally add by index to the original dataset.\n",
    "    \"\"\"\n",
    "    unknown = df[df.Age == 'unknown'].Title\n",
    "    classified = pd.merge(\n",
    "        unknown, clf, how='left', left_on='Title', right_index=True)\n",
    "    classified = classified.reindex(unknown.index)\n",
    "    df.loc[classified.index, 'Age'] = classified.clf\n",
    "    assert df[df.Age.isna()].empty\n",
    "    return df\n",
    "\n",
    "# Apply transformations\n",
    "train = df.copy()\n",
    "train = age_bins(train)\n",
    "train = cabins(train)\n",
    "train = fares(train)\n",
    "train = names(train)\n",
    "train = drop_useless_cols(train)\n",
    "train = classify_ages(train)\n",
    "\n",
    "# Apply transformations to test data\n",
    "test = testdf.copy()\n",
    "test = age_bins(test)\n",
    "test = cabins(test)\n",
    "test = fares(test)\n",
    "test = names(test)\n",
    "test = drop_useless_cols(test)\n",
    "test = classify_ages(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode JeffD likewise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Age', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Title']\n",
    "combined_df = pd.concat((train[features], test[features]))\n",
    "for feat in features:\n",
    "    le = LabelEncoder()\n",
    "    le = le.fit(combined_df[feat])\n",
    "    train.loc[:, feat] = le.transform(train[feat])\n",
    "    test.loc[:, feat] = le.transform(test[feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = train.drop(columns=['Survived'])\n",
    "y_all = train.Survived\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, train_size=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Random forest to classify the ages\n",
    "I tried a bayesian algorithm with normal distribution but it didn't work out.\n",
    "\n",
    "Now let's approach the ages with a random forest classifier like with Jeff notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the type of classifier. \n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Choose some parameter combinations to try\n",
    "parameters = {'n_estimators': [4, 6, 9], \n",
    "              'max_features': ['log2', 'sqrt','auto'], \n",
    "              'criterion': ['entropy', 'gini'],\n",
    "              'max_depth': [2, 3, 5], \n",
    "              'min_samples_split': [2, 3, 5],\n",
    "              'min_samples_leaf': [1,5,8]\n",
    "             }\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "acc_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer)\n",
    "grid_obj = grid_obj.fit(x_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data. \n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a single prediction\n",
    "Use our trained model on the test split to see its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(x_test)\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(10)\n",
    "outcomes = []\n",
    "fold = 0\n",
    "for train_idx, test_idx in kf.split(X_all):\n",
    "    fold += 1\n",
    "    X_train, y_train = X_all.values[train_idx], y_all.values[train_idx]\n",
    "    X_test, y_test = X_all.values[test_idx], y_all.values[test_idx]\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_hat = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_hat)\n",
    "    outcomes.append(accuracy)\n",
    "    print('Fold {} accuracy: {}'.format(fold, accuracy))\n",
    "np.mean(outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat= clf.predict(test)\n",
    "pd.Series(index=testdf.PassengerId, data=y_hat, name='Survived').to_csv(\n",
    "    'bayesian_age-3rd pass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
