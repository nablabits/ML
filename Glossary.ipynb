{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a quick index, copy the output into a MD cell.\n",
    "abc = 'A B C D E F G H I J K L M N O P Q R S T U V W X Y Z'\n",
    "arr = [' [{}](#{}) ·'.format(k, k) for k in abc.split()]\n",
    "print(''.join(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn algorithm cheatsheet\n",
    "Got from https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "![img](https://scikit-learn.org/stable/_static/ml_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glossary of terms\n",
    "Here are the terms collected during the learning journey.\n",
    "\n",
    "**Index**\n",
    "\n",
    "[A](#A) · [B](#B) · [C](#C) · [D](#D) · [E](#E) · [F](#F) · [G](#G) · [H](#H) · [I](#I) · [J](#J) · [K](#K) · [L](#L) · [M](#M) · [N](#N) · [O](#O) · [P](#P) · [Q](#Q) · [R](#R) · [S](#S) · [T](#T) · [U](#U) · [V](#V) · [W](#W) · [X](#X) · [Y](#Y) · [Z](#Z)\n",
    "\n",
    "***\n",
    "Also found the scikit learn glossary [sklearn](https://scikit-learn.org/stable/glossary.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B\n",
    "* **Bias:** the bias of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated.[[wiki]](https://en.wikipedia.org/wiki/Bias_of_an_estimator) [[Collective wisdom]](Collective%20wisdom/personal%20review.ipynb#A-BRIEF-PRIMER-ON-STATISTICAL-DECISION-THEORY)\n",
    "* **Bias-Variance tradeoff:** [[wiki]](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) Models with lower bias (nice) tend to have a higher variance (bad, overfitting) \n",
    "* **Black box model:** as opposite to [white box model](#W), those models in which we can't figure out what's going on, we know that they work fine but not why (eg, a neural network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C\n",
    "* **Classification:** the supervised learning where the target are categories (labeled data)\n",
    "\n",
    "* **Coefficient of determination:** $r^2$, a metric of how well performs an algorithm. Mathematically, is one minus the proportion between the sum of the squared residuals and the total sum of squares. [[wiki]](https://en.wikipedia.org/wiki/Coefficient_of_determination)\n",
    "\n",
    "* **Covariance matrix:** shows the multidimensional covariances (correlations). It generalizes the notion of variance to multiple dimensions. [[wiki]](https://en.wikipedia.org/wiki/Covariance_matrix)\n",
    "    \n",
    "     ![img](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/220px-GaussianScatterPCA.svg.png)\n",
    "    \n",
    "    Above picture are sample points from a bivariate Gaussian distribution with a standard deviation of 3 in roughly the lower left-upper right direction and of 1 in the orthogonal direction. Because the $x$ and $y$ components co-vary, the variances of $x$ and $y$ do not fully describe the distribution. A $2\\times 2$ covariance matrix is needed; the directions of the arrows correspond to the eigenvectors of this covariance matrix and their lengths to the square roots of the eigenvalues.  \n",
    "    \n",
    "   Covariance matrix is often built off a vector to show how related are components inside the vector. $cov (\\mathbf{X}, \\mathbf{X}) = E[(\\mathbf{X}-\\mu_X)(\\mathbf{X}-\\mu_X)]$. Notice the expectation across samples of same class. [[PCA visualized]](https://notsquirrel.com/pca/). This property can be used to guess values of a pixel's neigbors when knowing its value. Alternatively, when used as a linear transformation over an unknown image, it can be used to identify it depending the outcome of such transformation.\n",
    "\n",
    "* **Curse of dimensionality:** when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. [[wiki]](https://en.wikipedia.org/wiki/Curse_of_dimensionality) [[sklearn]](https://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html#the-curse-of-dimensionality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a covariance matrix over a vector\n",
    "\n",
    "# Create a column vector\n",
    "X = np.arange(5.).reshape(5, 1)\n",
    "\n",
    "# Compute the distance from the expected value\n",
    "X -= X.mean()\n",
    "\n",
    "# Finally build the matrix by dotting the col vector & its transposed\n",
    "np.dot(X, X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D\n",
    "* **Discretization:** reduce the number of values of a continuous feature by groupping them into intervals (bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E\n",
    "* **Eigenvalues:** the factor by which eigenvectors are scaled up or down.\n",
    "\n",
    "* **Eigenvectors:** when applying a linear transformation of the input space, are those vectors that will continue pointing in the same direction and they will be only scaled up or down.\n",
    "\n",
    "    When dealing with normal multivariate distributions the eigenvectors will be the axes of the ellipses "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F\n",
    "* **Fat tail:** some events in the real world are really prone to render very unexpected outcomes with some probability way greater than if they were normally distributed events. These events lay 3 or more standard deviations from the mean event and they usually appear as outliers in the plots (like whisker plots). These kind of events, might be the hardest to forecast. [[wiki]](https://en.wikipedia.org/wiki/Fat-tailed_distribution)\n",
    "\n",
    "* **Feature:** variable, column in a dataset, dimension, all of these mean the same concept\n",
    "\n",
    "* **Fit (method):** fit method calculates the algorithm, that is, creates the model that will be used for predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H\n",
    "* **Hiperplane:** a subspace whose dimension is one less than that of its ambient space. [[wiki]](https://en.wikipedia.org/wiki/Hyperplane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I\n",
    "* **Inverse problem:** watching some random datapoints come up with the mathematical model (if any) that could generate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## J\n",
    "* **Jensen inequalities:** The loss of the expectation is allways less or equal to the expectation of losses. [[Collective Wisdom]](Collective%20wisdom/personal%20review.ipynb#Jensen-inequalities)\n",
    "\n",
    "\n",
    "* **Joint probability:** Understand the intersection of probabilities, the probability of a bunch events happening together whether independent or dependent. \n",
    "[[source]](https://www.statisticshowto.datasciencecentral.com/joint-probability-distribution/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K\n",
    "* **Kernel function:** a symmetric function over pairs of data points. Those pairs are: \n",
    "    1. The training instances, and\n",
    "    2. The testing ones.  \n",
    "    \n",
    "  Outputs a number, the distance between the two points, which is used in turn by a *kernel method* [wiki](https://en.wikipedia.org/wiki/Positive-definite_kernel)\n",
    "* **Kernel method:** Predicts the value by the unlabeled point in the kernel method by summing the weighted kernels from all the points trained to the testing point so the farther they are the less influence they'll have [wiki](https://en.wikipedia.org/wiki/Kernel_method) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L\n",
    "* **L1 norm:** metric in which the distance between two points is the sum of the absolute differences of their Cartesian coordinates. Can also refer to L1 loss function:\n",
    "\n",
    "* **L1-norm loss function**: also known as least absolute deviations (LAD), least absolute errors (LAE). It is basically minimizing the sum of the absolute differences (S) between the target value ($Y_i$) and the estimated values ($f(x_i)$) [[s]](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)\n",
    "\n",
    "\n",
    "* **L2 norm:** or euclidean norm, the distance is given by pythagoras. Can also be applied to the least squares distance loss function:\n",
    "\n",
    "* **L2-norm loss function:** is also known as least squares error (LSE). It is basically minimizing the sum of the square of the differences (S) between the target value ($Y_i$) and the estimated values ($f(x_i)$) [[s]](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)\n",
    "\n",
    "**L1-L2 comparison**\n",
    "\n",
    "![img](https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/Manhattan_distance.svg/200px-Manhattan_distance.svg.png)\n",
    "\n",
    "* **Likelihood:**  Likelihood is a measure of the extent to which a sample provides support for particular values of a parameter in a parametric model. [[source]](https://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability). \n",
    "\n",
    "    In probability we start with a model defined by some parameters (like mean and deviation) to get samples i.e. a PDF. However, in statistics,  we start from the observations and try to come up with the model parameters (Inverse problem). \n",
    "    \n",
    "    Also, in Bayes' theorem, is the likelihood of the data given some hypothesis.\n",
    "    \n",
    "* **Likelihood function:** The function (a pdf or a pmf) that we assume generated the data we have. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M\n",
    "* **Multi-class classification:** the model's vectors we are going to work with, have more than two dimensions.\n",
    "* **Mutual information:** or info gain. A metric that shows how much information of a variable *A* is contained in another variable *B*. So the lower the number (close to 0) the more independent the variables are.\n",
    "* **Maximum likelihood estimation:** try to come up with the parameters that maximize the [likelihood function](#L) for the given datapoints. That is performed by finding the joint probability of all datapoints for that likelihood function and then maximizing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N\n",
    "* **Normalization:** the process of scaling individual samples to have unit norm, that is, they will be in the range {0, 1} or {-1, 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O\n",
    "* **One vs one:** when we deal with multi-class classification, that is, our input vectors have more than two dimensions, is the strategy where we evaluate the classes in pairs. Say we have three classes, A, B and C. The OVO ensemble will be composed of 3 (= 3 * (3 - 1) / 2) binary classifiers. The first will discriminante A from B, the second A from C, and the third B from C. At prediction stage, the class that got the highest number of \"+1\" predictions is our winner.\n",
    "* **One vs rest:** (aka one-vs-all),when we deal with multi-class classification, is the strategy that involves training one classifier (estimator) for class and then taking the one which gives the highest confidence. [wiki](https://en.wikipedia.org/wiki/Multiclass_classification#Transformation_to_binary)\n",
    "* **Outliers:** An outlier is an observation that lies an abnormal distance from other values in a random sample from a population [[s]](https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm). Normal distributions don't get along well enough with them [[wiki]](https://en.wikipedia.org/wiki/Linear_discriminant_analysis#Assumptions)\n",
    "* **Overfitting:** models that do not generalise the data well, that is, they stick to the dataset learned so much that they don't work well with other datasets or predict future observations realiably.  \n",
    "On the following graph the green line represents an overfitted model and the black line represents a regularized model. While the green line best follows the training data, it is too dependent on that data and it is likely to have a higher error rate on new unseen data, compared to the black line.\n",
    "\n",
    "![img](https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/300px-Overfitting.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P\n",
    "* **Pipeline:** the stages in a machine learning project. Usually they are:\n",
    "    * Preprocessing\n",
    "    * Feature selection\n",
    "    * ML algorithm\n",
    "    * Model\n",
    "    * Evaluation\n",
    "* **Preprocessing:** the stage where we clean, prepare and make sense of the data we'll work with.\n",
    "* **Principal component analysis:** a techique that let's us to get a reduced & efficient version of a given matrix (sometimes covariance matrix?), that can be used in turn for predictions. Efficient since provides eigenvectors (unique information) and reduced since only a few of these eigenvectors are enough to give close approximations to the original matrix. \n",
    "* **Probability distribution:** the mathematical function that provides the probabilities of different possible outcomes of a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R\n",
    "* **Regression:** Is a broad concept. Is the set of statistical processes for estimating the relationships between a dependent variable *--target--* and one or more independent variables *--predictors, covariates or features--*. Is widely used for prediction and forecasting.\n",
    "\n",
    "  The basic idea of regression is the following:\n",
    "  \n",
    "  $$\\displaystyle\\hat{y}(\\mathbf{w},\\mathbf{x})=w_0+\\mathbf{w_1 x_1}+...+\\mathbf{w_p y_p}$$\n",
    "  We'll try to predict a $\\hat{y}$ by assigning a coeficient ($\\mathbf{w}$, weight) to each component (feature) of the vector $\\mathbf{X}$ we input and an intercept point (constant term) $w_0$.\n",
    "\n",
    "  That is: we assume that **every target in the data can be approximated by a linear combination of its features.**\n",
    "  \n",
    "  Also, it could be the family of algorithms used in supervised learning where the targets are continous values (rather than labels/categories). Although the logistic regression deals with classification problems.\n",
    "  \n",
    "* **Regularization:** the process by which we add information to a model to prevent overfitting [[wiki]](https://en.wikipedia.org/wiki/Regularization_(mathematics)).   \n",
    "    \n",
    "    Also in SVC algorithms is a parameter that provides a well-defined solution [[wiki]](https://en.wikipedia.org/wiki/Support-vector_machine#Regularization_and_stability).\n",
    "  \n",
    "* **Residual:** The distance between the estimated value and the real value not confuse with statistical errors [[wiki]](https://en.wikipedia.org/wiki/Errors_and_residuals). Uses:\n",
    "    * Give a feel whether the data has a linear behavior\n",
    "    * we want them to be normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S\n",
    "* **Singular value descomposition (SVD):** Any linear transformation can be defined as a three component transformations, say:  \n",
    "    * Initial rotation $\\mathbf{V^T}$  \n",
    "    * Axis-wise scaling $\\mathbf{\\Sigma}$  \n",
    "    * Final rotation $\\mathbf{U}$\n",
    "    \n",
    "    $\\displaystyle\\mathbf{M} = \\mathbf{U}\\cdot\\mathbf{\\Sigma}\\cdot\\mathbf{V^T}$\n",
    "    \n",
    "    [[wiki]](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
    "    \n",
    "    ![SVD](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/Singular-Value-Decomposition.svg/500px-Singular-Value-Decomposition.svg.png)\n",
    "\n",
    "* **Supervised Learning:** the data also comes with an output (also known class column). It involves either classification (classes are categories) or regression methods (classes are numbers)\n",
    "\n",
    "* **Standarization:** the process of rescaling the features so that they’ll have the properties of a Gaussian distribution with μ=0 and σ=1 where μ is the mean and σ is the standard deviation from the mean\n",
    "\n",
    "* **Statistical errors:** are the deviations from the observed value and the unobsevable true value [[wiki]](https://en.wikipedia.org/wiki/Errors_and_residuals)\n",
    "\n",
    "* **Support vector:** the data points that are closer to the hyperplane and influence its orientation  and position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T\n",
    "* **Transform and compare:** on this machine learning problems is quite usual to find the following operation: $\\mathbf{x}^t\\cdot\\mathbf{M}\\cdot\\mathbf{x}$ whose intuituition is *take a vector, apply a linear transformation to it and compare the outcome with the original.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U\n",
    "* **Unsupervised learning:** the data doesn't come with any outcome (no class column) so it is used in a exploratory way. [Mathworks](https://www.mathworks.com/discovery/unsupervised-learning.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V\n",
    "* **Variance:** measures the spread of the data in a sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W\n",
    "* **White box model:** those models on which we can figure out what's going on inside. The opposite of a [black box model](#B)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X\n",
    "* **$X$:** all the data matrix, input vectors. $X_i$, some instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y\n",
    "* **$y$:** the target column or output, that is, the real outcome we get when we input $X$. $y_i$, some instance.\n",
    "* **$\\hat{y}$:** a prediction of the learned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
