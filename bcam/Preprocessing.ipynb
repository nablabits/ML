{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary \n",
    "\n",
    "This is the summary of the crash course took at [BCAM](http://www.bcamath.org/en/) on 2019 40th week\n",
    "\n",
    "\n",
    "**Index**  \n",
    "* [Introduction](#Introduction)  \n",
    "* [Preprocessing](#Preprocessing)\n",
    "    * [Outliers](#Section-A%3A-Outliers)\n",
    "    * [NaN values](#Section-B%3A-NaN-values)\n",
    "    * [Discretiaztion](##Section-D%3A-discretization)\n",
    "    * [Feature Selection](#Section-D%3A-Feature-selection)\n",
    "    * [Feature Extraction](#Section-E%3A-feature-extraction.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "There are three steps when it comes to ML:\n",
    " * Preprocess: is about ~80% of the time, we prepare and clean the data we are about to work with. In this stage we also select the features we'll work with.\n",
    " * Process: Where the learning takes place.\n",
    " * Evaluating: measuring the quality of our predictions.\n",
    " \n",
    "We can come across with two kind of data: the one that shows both input variables and the output for them (categorical or numerical), and the one that only shows the inputs and we must infer the outputs. The learning that deals with first one is called **supervised learning** [wiki](https://en.wikipedia.org/wiki/Supervised_learning), whereas the one that deals with the latter is **non-supervised learning.** [mathworks](https://www.mathworks.com/discovery/unsupervised-learning.html)\n",
    "\n",
    "Supervised learning works mainly in classification (the outputs are categories) & regression (the outputs are numbers).\n",
    "\n",
    "Unsupervised learning deals with clustering, aka, group similar elements together.\n",
    "\n",
    "We were also playing with the iris dataset in scikit learn.\n",
    "Quite interesting the pairplot method of seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting familiar with the data\n",
    "The following cells are a sample to get familiar with the data we'll work with. Also remember that expert knowledge is quite relevant, so always try to stay in contact with the data providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, KBinsDiscretizer, LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (a set of numpy arrays)\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Convert to a DF\n",
    "iris = pd.DataFrame(data= np.c_[iris['data'], iris['target']],columns= iris['feature_names'] + ['target'])\n",
    "iris['target'] = iris['target'].astype(int)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a quick summarizing view about the data\n",
    "iris.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pair plots for each column in the dataset to find out relationships\n",
    "sns.pairplot(iris, vars=iris.columns[:-1], hue='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice several things here:\n",
    "* the target 0 (Setosa) is fairly different from the others. Their petal dimesions are significantly smaller than the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The violin plot in seaborn is mix between a box plot and a kernel density function\n",
    "fig, ax = pyplot.subplots(figsize =(9, 7)) \n",
    "sns.violinplot( ax = ax, y = iris[\"petal width (cm)\"], x = iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "**Preprocessing** is the stage in a machine learning pipeline where we clean and make sense of the data we have. It takes the **80%** of the time spent in a ML project. THe bigger the data the more important its preprocessing process.\n",
    "\n",
    "**Expert knowledge** is always relevant. It provides the most important features, the ranges for variables, helps identifying redundancies and set bounds to the modeling choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section A: Outliers\n",
    "Outliers are often bad data points [1](https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm). \n",
    "They can be detected collectivelly (using Mahalanobis distance) or individually, that is, feature by feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df=pd.read_csv('iris_data.csv')\n",
    "\n",
    "# Get a copy to work with\n",
    "df_A = df.copy()\n",
    "\n",
    "# let's look for outliers into the whole data (uncomment to see)\n",
    "ax = sns.boxplot(data=df_A[df_A.columns[:-1]], orient=\"h\", palette=\"Set2\", linewidth=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we see that petal_width has some of them, let's plot it\n",
    "ax2 = sns.boxplot(data=df_A['sepal_width'], orient=\"h\", color=sns.color_palette(\"Set2\")[1], linewidth=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we're going to get rid of them\n",
    "# First, define interquartile range\n",
    "df_A_stats=df_A.describe()\n",
    "IQR = df_A_stats[\"sepal_width\"][\"75%\"] - df_A_stats[\"sepal_width\"][\"25%\"]\n",
    "\n",
    "# And the whiskers\n",
    "whiskers = [df_A_stats[\"sepal_width\"][\"25%\"]-(1.5*IQR),\n",
    "            df_A_stats[\"sepal_width\"][\"75%\"]+(1.5*IQR)]\n",
    "\n",
    "# Get the outliers\n",
    "outliers=df_A[\n",
    "    (df_A['sepal_width'] > whiskers[1]) |\n",
    "    (df_A['sepal_width'] < whiskers[0])\n",
    "]\n",
    "\n",
    "# Now, drop'em all\n",
    "data1_outliers=df_A.drop(index=outliers.index)\n",
    "assert(data1_outliers.shape[0] == 146)  # Four indices were dropped as expected\n",
    "\n",
    "# Finally plot the graph to ensure everything was fine\n",
    "ax2 = sns.boxplot(y=\"sepal_width\", data=data1_outliers, orient=\"h\", color=sns.color_palette(\"Set2\")[1], linewidth=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B: NaN values\n",
    "\n",
    "When coming across with NaNs, two approaches can be taken:\n",
    "* If they are few of the total dataset and the dataset is large, we can remove them.\n",
    "* Otherwise they can be guessed by especific algorithms.\n",
    "\n",
    "### Getting rid of NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get a copy of the original dataframe\n",
    "df_nan = df.copy()\n",
    "\n",
    "# Create some nans\n",
    "df_nan.iloc[1, 0] = np.nan\n",
    "df_nan.iloc[5:7, 1] = np.nan\n",
    "df_nan.iloc[7, 3] = np.nan\n",
    "\n",
    "# Get rid of nan values and reindex the df\n",
    "df_not_nan = df_nan.dropna().reset_index()\n",
    "\n",
    "# There should be no NaNs\n",
    "assert(((df_not_nan != np.isnan).any()).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIlling in the NaNs with the mean value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an  array of the values\n",
    "X_nan = df_nan.iloc[:, :-1].values\n",
    "\n",
    "# Now instantiate the imputer from sklearn\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# Calculate for X_nan and transform the original values\n",
    "X_imputed = imp.fit(X_nan).transform(X_nan)\n",
    "\n",
    "# Check that the first value imputed actually has the mean value in the previous dataset\n",
    "assert(df_nan.iloc[:,0].mean() == X_imputed[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section C: Standarization & normalization\n",
    "*From the slides*\n",
    "* When  computing  distances  between  pairs  of  samples,  the  scales  of  the different features is very relevant.\n",
    "\n",
    "* Moreover, we cannot obviate the curse of dimensionality effect, i.e. in high-dimensional spaces all data is sparse. In simple words, all distances become huge.\n",
    "\n",
    "* Therefore, if  the  algorithm  we  plan  to  apply  after  preprocessing  implies distances and/or we are in a high-dimensional problem, we should transform all the features to a similar scale.\n",
    "\n",
    "***\n",
    "**Standarization:** Standardization (or Z-score normalization) is the process of rescaling the features so that they’ll have the properties of a Gaussian distribution with μ=0 and σ=1 where μ is the mean and σ is the standard deviation from the mean\n",
    "\n",
    "**Normalization:** the process of scaling individual samples to have unit norm, that is, they will be in the range {0, 1} or {-1, 1}. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples. [s](https://scikit-learn.org/stable/modules/preprocessing.html#normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new copy of the data\n",
    "data2=df.copy()\n",
    " \n",
    "# Split into input space and output space\n",
    "X2 = data2[['sepal_length','sepal_width', 'petal_length', 'petal_width']]\n",
    "y2 = data2['species']\n",
    "\n",
    "# Instantiate the standard scaler (standarization)\n",
    "sc = StandardScaler()\n",
    "X2_standarized = sc.fit_transform(X2)\n",
    "\n",
    "# Ensure that the values are --almost- 0 & 1 respectivelly\n",
    "assert((X2_standarized.mean(axis=0) < 1e-10).all())\n",
    "assert((X2_standarized.std(axis=0) < 1 + 1e-10).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the MinMaxScaler & apply\n",
    "sc = MinMaxScaler()\n",
    "X2_normalized = sc.fit_transform(X2)\n",
    "\n",
    "# Ensure max=1 & min=0\n",
    "assert((X2_normalized.min(axis=0) == 0).all())\n",
    "assert((X2_normalized.max(axis=0) == 1).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section D: discretization\n",
    "\n",
    "*From the slides*\n",
    "* The goal of discretization is reducing the number of values of a continuous attribute by grouping them into intervals (bins). The new values are the bins.\n",
    "\n",
    "* Some methods require discrete attributes (some naïve Bayes and Bayesian networks methods, etc.).\n",
    "\n",
    "* Sometimes,  the results  are better  after discretization. Some methods do it implicitly (e.g. decision trees).\n",
    "\n",
    "* In  general,  the  computational  cost  of  algorithms  with  discrete  attributes  is lower than their continuous versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new copy of the data to play with\n",
    "data3=df.copy()\n",
    "\n",
    "# Select the features we'll work with\n",
    "X3 = data3[['sepal_length','sepal_width', 'petal_length', 'petal_width']]\n",
    "\n",
    "# Instantiate the discretizers: equal width & equal frequency\n",
    "enc1 = KBinsDiscretizer(n_bins=[3, 4, 2, 2], encode='ordinal', strategy='uniform')\n",
    "enc2 = KBinsDiscretizer(n_bins=[3, 4, 2, 2], encode='ordinal', strategy='quantile')\n",
    "\n",
    "# Calcualte the bins and transform them\n",
    "X_binned_EW = enc1.fit(X3).transform(X3)\n",
    "X_binned_EF = enc2.fit(X3).transform(X3)\n",
    "\n",
    "# Now plot them to see the differences\n",
    "fig =plt.figure(figsize=(20, 20))\n",
    "\n",
    "plt.subplot(421)\n",
    "plt.hist(X_binned_EW[:,0])\n",
    "\n",
    "plt.title('Equal-Width -> sepal_length',fontsize=20)\n",
    "\n",
    "plt.subplot(422)\n",
    "plt.hist(X_binned_EF[:,0])\n",
    "plt.title('Equal-Frequency -> sepal_length',fontsize=20)\n",
    "\n",
    "plt.subplot(423)\n",
    "plt.hist(X_binned_EW[:,1])\n",
    "plt.title('Equal-Width -> sepal_width',fontsize=20)\n",
    "\n",
    "plt.subplot(424)\n",
    "plt.hist(X_binned_EF[:,1])\n",
    "plt.title('Equal-Frequency -> sepal_width',fontsize=20)\n",
    "\n",
    "plt.subplot(425)\n",
    "plt.hist(X_binned_EW[:,2])\n",
    "plt.title('Equal-Width -> petal_length',fontsize=20)\n",
    "\n",
    "plt.subplot(426)\n",
    "plt.hist(X_binned_EF[:,2])\n",
    "plt.title('Equal-Frequency -> petal_length',fontsize=20)\n",
    "\n",
    "plt.subplot(427)\n",
    "plt.hist(X_binned_EW[:,3])\n",
    "plt.title('Equal-Width -> petal_width',fontsize=20)\n",
    "\n",
    "plt.subplot(428)\n",
    "plt.hist(X_binned_EF[:,3])\n",
    "plt.title('Equal-Frequency -> petal_width',fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section D: Feature selection\n",
    "\n",
    "Dealing with all the features often is not feasible so, therefore, we'll have to select the features we'll work with.\n",
    "\n",
    "We can select the features using three kinds of methods:  \n",
    "* Filter methods: based on intrinsic properties of the data, such as correlation or mutual information.\n",
    "\n",
    "* Wrapper methods: using some learning task in the selection that will be likely used in the subsequent processing.\n",
    "\n",
    "* Embedded methods: those included in the learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get a copy of the data\n",
    "data4=df.copy()\n",
    "\n",
    "# we can see the width_and length have a high correlation\n",
    "# so we can get rid of one of them\n",
    "data4.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutual information or info gain\n",
    "X4 = data4[['sepal_length','sepal_width', 'petal_length', 'petal_width']].values\n",
    "y4 = data4['species']\n",
    "\n",
    "# Encoding the Dependent Variable\n",
    "y4 = LabelEncoder().fit_transform(y4)\n",
    "\n",
    "mi=mutual_info_classif(X4, y4_num)\n",
    "mi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section E: feature extraction.\n",
    "* Feature extraction is the action of defining new features from all or part of the original ones.\n",
    "\n",
    "* Despite  expert  knowledge  could  be  also  used,  we  focus  on  data-driven approaches.\n",
    "\n",
    "* When applying feature extraction, there is usually a price to pay in terms of loss  of  interpretability because  of  the  lack  of  meaning  of  the  artificial features.\n",
    "\n",
    "* There are both supervised and unsupervised methods.\n",
    "\n",
    "* We will have a deeper look to the most famous unsupervised (and linear) method: principal component analysis (PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component analysis\n",
    "\n",
    "Take the features and compress them in a lower dimension matrix by a method where the greatest variance by some projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.\n",
    "\n",
    "From the scikit-learn main page: https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html\n",
    "\n",
    "Principal Component Analysis (PCA) applied to this data identifies the combination of attributes (principal components, or directions in the feature space) that account for the most variance in the data. Here we plot the different samples on the 2 first principal components.\n",
    "\n",
    "Linear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance between classes. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Get Principal componen analysis\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit(X).transform(X)\n",
    "\n",
    "# Now, the linear discriminant analysis\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_r2 = lda.fit(X, y).transform(X)\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.figure()\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "lw = 2\n",
    "\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA of IRIS dataset')\n",
    "\n",
    "plt.figure()\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('LDA of IRIS dataset')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
