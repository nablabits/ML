{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [A](#A) · [B](#B) · [C](#C) · [D](#D) · [E](#E) · [F](#F) · [G](#G) · [H](#H) · [I](#I) · [J](#J) · [K](#K) · [L](#L) · [M](#M) · [N](#N) · [O](#O) · [P](#P) · [Q](#Q) · [R](#R) · [S](#S) · [T](#T) · [U](#U) · [V](#V) · [W](#W) · [X](#X) · [Y](#Y) · [Z](#Z) ·\n"
     ]
    }
   ],
   "source": [
    "# Create a quick index, copy the output into a MD cell.\n",
    "abc = 'A B C D E F G H I J K L M N O P Q R S T U V W X Y Z'\n",
    "arr = [' [{}](#{}) ·'.format(k, k) for k in abc.split()]\n",
    "print(''.join(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glossary of terms\n",
    "Here are the terms collected during my learning journey.\n",
    "\n",
    "**Index**\n",
    "\n",
    "[A](#A) · [B](#B) · [C](#C) · [D](#D) · [E](#E) · [F](#F) · [G](#G) · [H](#H) · [I](#I) · [J](#J) · [K](#K) · [L](#L) · [M](#M) · [N](#N) · [O](#O) · [P](#P) · [Q](#Q) · [R](#R) · [S](#S) · [T](#T) · [U](#U) · [V](#V) · [W](#W) · [X](#X) · [Y](#Y) · [Z](#Z)\n",
    "\n",
    "***\n",
    "Also found the scikit learn glossary [sklearn](https://scikit-learn.org/stable/glossary.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C\n",
    "* **Curse of dimensionality:** when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. [[s]](https://en.wikipedia.org/wiki/Curse_of_dimensionality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D\n",
    "* **Discretization:** reduce the number of values of a continuous feature by groupping them into intervals (bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F\n",
    "* **Feature:** variable, column in a dataset, dimension, all of these mean the same concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K\n",
    "* **Kernel function:** a symmetric function over pairs of data points. Those pairs are: \n",
    "    1. The training instances, and\n",
    "    2. The testing ones.  \n",
    "    \n",
    "  Outputs a number, the distance between the two points, which is used in turn by a *kernel method* [wiki](https://en.wikipedia.org/wiki/Positive-definite_kernel)\n",
    "* **Kernel method:** Predicts the value by the unlabeled point in the kernel method by summing the weighted kernels from all the points trained to the testing point so the farther they are the less influence they'll have [wiki](https://en.wikipedia.org/wiki/Kernel_method) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L\n",
    "* **L1 norm:** metric in which the distance between two points is the sum of the absolute differences of their Cartesian coordinates. Can also refer to L1 loss function:\n",
    "\n",
    "* **L1-norm loss function**: also known as least absolute deviations (LAD), least absolute errors (LAE). It is basically minimizing the sum of the absolute differences (S) between the target value ($Y_i$) and the estimated values ($f(x_i)$) [[s]](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)\n",
    "\n",
    "\n",
    "* **L2 norm:** or euclidean norm, the distance is given by pythagoras. Can also be applied to the least squares distance loss function:\n",
    "\n",
    "* **L2-norm loss function:** is also known as least squares error (LSE). It is basically minimizing the sum of the square of the differences (S) between the target value ($Y_i$) and the estimated values ($f(x_i)$) [[s]](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)\n",
    "\n",
    "**L1-L2 comparison**\n",
    "\n",
    "![img](https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/Manhattan_distance.svg/200px-Manhattan_distance.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N\n",
    "* **Normalization:** the process of scaling individual samples to have unit norm, that is, they will be in the range {0, 1} or {-1, 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O\n",
    "* **Outliers:** An outlier is an observation that lies an abnormal distance from other values in a random sample from a population [[s]](https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P\n",
    "* **Pipeline:** the stages in a machine learning project. Usually they are:\n",
    "    * Preprocessing\n",
    "    * Feature selection\n",
    "    * ML algorithm\n",
    "    * Model\n",
    "    * Evaluation\n",
    "* **Preprocessing:** the stage where we clean, prepare and make sense of the data we'll work with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S\n",
    "* **Supervised Learning:** the data also comes with an output (also known class column). It consists either in classification (outputs are categories) or in regression methods (outputs are numbers)\n",
    "\n",
    "* **Standarization:** the process of rescaling the features so that they’ll have the properties of a Gaussian distribution with μ=0 and σ=1 where μ is the mean and σ is the standard deviation from the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U\n",
    "* **Unsupervised learning:** the data doesn't come with any outcome (no class column) so it is used in a exploratory way. [Mathworks](https://www.mathworks.com/discovery/unsupervised-learning.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
