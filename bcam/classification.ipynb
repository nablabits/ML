{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "* [About classification algorithms](#About-classification-algorithms)\n",
    "* [Train and test subsamples](#Train-and-test-subsamples)\n",
    "* [K nearest neighbors](#KNN)\n",
    "* [Decision Trees](#Decision-Trees)\n",
    "    * [Splitting the dataset](#Splitting-the-dataset)\n",
    "    * [Avoiding overfitting](#Avoiding-overfitting)\n",
    "* [Logistic regression](#Logistic-Regression) \n",
    "    * [Example #1: random samples](#Example-#1,-logistic-regresion-with-random-samples)\n",
    "    * [Example #2: iris dataset](#Example-#2,-logistic-regresion-with-iris-dataset)\n",
    "* [Discriminant analysis](#Discriminant-analysis)\n",
    "* [Naive Bayes](#Naive-Bayes)\n",
    "* [Support vector machines (SVM)](#Support-Vector-Machines)\n",
    "* [PCA for classification](#PCA-for-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About classification algorithms\n",
    "Some classification algorithms can only distinguish between two classes, how can we use them in multi class problems? There are two approaches to this:\n",
    "    \n",
    "* **One vs one:** is the approach where we evaluate the classes in pairs. Say we have three classes, A, B and C. The OVO ensemble will be composed of 3 (= 3 * (3 - 1) / 2) binary classifiers. The first will discriminante A from B, the second A from C, and the third B from C. At prediction stage, the class that got the highest number of \"+1\" predictions is our winner. Notice that this is a $O(n^2)$ problem\n",
    "      \n",
    "* **One vs rest:** (aka one-vs-all)is the strategy that involves training one classifier (estimator) for class and then taking the one which gives the highest confidence.\n",
    "\n",
    "[wiki](https://en.wikipedia.org/wiki/Multiclass_classification#Transformation_to_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test subsamples\n",
    "In general we should split the data given in two parts: one for training and the other for testing. Usually the testing slice is 1/3 of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import (\n",
    "    ListedColormap, LinearSegmentedColormap, Normalize, )\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "# Some estimators\n",
    "from sklearn import neighbors, datasets, linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis, )\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Import some utils\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "# import some data to play with\n",
    "IRIS = datasets.load_iris()\n",
    "\n",
    "# Pandas version\n",
    "IRIS_PD = pd.DataFrame(data= np.c_[IRIS['data'], IRIS['target']],columns= IRIS['feature_names'] + ['target'])\n",
    "IRIS_PD['target'] = IRIS_PD['target'].astype(int)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Some useful functions\n",
    "\n",
    "def mesh(X, h=.01):\n",
    "    \"\"\"Create a meshgrid object with the input space dimensions.\"\"\"\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return (xx, yy)\n",
    "\n",
    "def quick_scatter(X, y):\n",
    "    \"\"\"Create a quick scatter plot.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.scatter(X, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "K nearest neighbors is a *lazy* algorithm which does not learn and makes computations in classification time, that is, find a predefined number of training samples (k) closest in distance to the new point, and predict the label from these.\n",
    "\n",
    "Notice that KNN takes by default the k **closest samples regardless how far they are**, to mitigate this effect a weight parameter can be added.\n",
    "\n",
    "KNN can also be applied to time series but they're pretty much regression problems we'll see them in due time. \n",
    "\n",
    "**Key features of KNN:**\n",
    "* Easy to understand and implement.\n",
    "* Computationally efficient in general (with small datasets).\n",
    "* Defining similarities.\n",
    "* The first thing that should be tried when approaching a ML problem.\n",
    "* They suffer especially the [curse of dimensionality](./../Glossary.ipynb/#C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### PART #1, Load an preprocess the data #####\n",
    "\n",
    "# we only take the first two features in the dataset\n",
    "X = IRIS.data[:, :2]\n",
    "cols = IRIS['feature_names'][:2]\n",
    "y = IRIS.target\n",
    "\n",
    "\n",
    "##### PART 2, create the model #####\n",
    "\n",
    "# Number of neighbors and weight\n",
    "k, w = 30, 'distance'\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors=k, weights=w)\n",
    "clf.fit(X, y)\n",
    "\n",
    "\n",
    "##### PART #3, plot the outcome ####\n",
    "\n",
    "# We are about to create a mesh of points that will represent a bunch of predictions \n",
    "xx, yy = mesh(X)\n",
    "    \n",
    "# Once created the mesh, drop all the points into the model and predict the values for them\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Plot the prediction areas (background)\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "Z = Z.reshape(xx.shape)  # reshape to match the grid, same as yy.shape\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points (real points)\n",
    "cmap_bold =  ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n",
    "plt.xlabel(cols[0])\n",
    "plt.ylabel(cols[1])\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = %i)\"% (k))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "[Nice visualization](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/) \n",
    "\n",
    "Decision trees divide the space into high dimensional rectangles. They are simple to understand and interpret (white box model), but they tend to overfit the data. However, they are useful in other ML techniques like bagging or random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# We take the features in pairs (Uncomment to see other pairs)\n",
    "pair = [0,1]\n",
    "#pair = [1,2] \n",
    "#pair = [2,3] \n",
    "X = IRIS.data[:, pair]\n",
    "y = IRIS.target\n",
    "\n",
    "# Train\n",
    "clf = DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "# Display the score (in the same training set, notice)\n",
    "print('score was: {}'.format(clf.score(X, y)))\n",
    "\n",
    "# Again, create a mesh of points that will represent a bunch of predictions\n",
    "xx, yy = mesh(X)\n",
    "\n",
    "# Once created the mesh, drop all the points into the model and predict the values for them\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "plt.xlabel(IRIS.feature_names[pair[0]])\n",
    "plt.ylabel(IRIS.feature_names[pair[1]])\n",
    "\n",
    "# Plot the training points\n",
    "plot_colors, n_classes = 'ryb', 3\n",
    "for i, color in zip(range(n_classes), plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=IRIS.target_names[i], edgecolor='black', s=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n",
    "Let's split the dataset into training and testing subsamples so we can check how effective is our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y)\n",
    "\n",
    "# Train\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Display the score\n",
    "print('score was: {}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "# Plot the decision boundary\n",
    "xx, yy = mesh(X)\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "plt.xlabel(IRIS.feature_names[pair[0]])\n",
    "plt.ylabel(IRIS.feature_names[pair[1]])\n",
    "\n",
    "# Plot the training points\n",
    "for i, color in zip(range(n_classes), plot_colors):\n",
    "    idx = np.where(y_test == i)\n",
    "    plt.scatter(X_test[idx, 0], X_test[idx, 1], c=color, label=IRIS.target_names[i], edgecolor='black', s=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding overfitting\n",
    "As we can see, decision trees tend to overfit to the trining data, there are two ways to mitigate this:\n",
    "* Bagging **(B**ootstrap **agg**regat**ing**) [[wiki]](https://en.wikipedia.org/wiki/Bootstrap_aggregating)\n",
    "* Random Forests [[wiki]](https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "**A: Bagging:** take several random subsets of the data, train them independently and finally aggregate the resutls and vote the best one.\n",
    "\n",
    "**B: Random Forests:** like above but instead of taking random subsets of the data, we take random subsets of the features. That prevents errors produced by correlations in the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create an artificial dataset\n",
    "X, y = datasets.make_classification(n_samples=10000, n_features=6,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "\n",
    "# Now split the train and the test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y)\n",
    "\n",
    "# Train the model\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=100, max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)  \n",
    "\n",
    "# Display the score\n",
    "print('score was: {}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "print(clf.predict([[0, 0, 0, 0, 0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "[Regression](https://en.wikipedia.org/wiki/Regression_analysis) is a wide topic in maths and machine learning that tries to estimate an outcome (target) given several independent variables called predictors or features so we can forecast a future output.\n",
    "\n",
    "The basic idea of regression is the following:\n",
    "\n",
    "\n",
    "$$\\hat{y}(\\mathbf{w},\\mathbf{x})=w_0+\\mathbf{w_1 x_1}+...+\\mathbf{w_p y_p}$$\n",
    "\n",
    "\n",
    "We'll try to predict a $\\hat{y}$ by assigning a coeficient ($\\mathbf{w}$, weight) to each component (feature) of the vector $\\mathbf{X}$ we input and an intercept point (constant term) $w_0$.\n",
    "\n",
    "That is: we assume that **every target in the data can be approximated by a linear combination of its features.**\n",
    "\n",
    "In the case of logistic regression, we can plug this line as an argument of the logistic function to get a probability for a certain sample $X$ to be classified as 0 or 1. Odds under 2:2 will be classified as $0$ and $1$ otherwise.   \n",
    "\n",
    "**Consider:** \n",
    "* When the classes are well separated can be unstable: if there's a feature that separates classes perfectly the coefficients go up to infinity.\n",
    "\n",
    "* If the sample is small, discriminant analysis is more accurate.\n",
    "\n",
    "\n",
    "#### **Example #1, logistic regresion with random samples**\n",
    "An adaptation from a [[scikit-learn]](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html#sphx-glr-auto-examples-linear-model-plot-logistic-py) example. \n",
    "\n",
    "* **Black dots:** a random sample where values over 0 yield $y=1$ (mostly but not always) and $y=0$ otherwise.\n",
    "\n",
    "* **Blue line:** a logistic model that predict a certain value. Every prediction for $X>0.222$ ($-w_0/w_1$) will be classified as $1$ otherwise $0$.\n",
    "\n",
    "* **Red curve:** the probabilty that the prediction will be 1. Notice that the point where probability is $0.5$ is precisely $-w_0/w_1$\n",
    "\n",
    "There is also a [Desmos graph](https://www.desmos.com/calculator/binjtdtjry) to get a feeling of how those coefficients affect to the final logistic curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of 300 random samples cetered at 0 \n",
    "n_samples = 300\n",
    "np.random.seed(0)\n",
    "X = np.random.normal(size=n_samples)\n",
    "\n",
    "# Now, add value 1 to samples over 0 and 0 to samples under 0\n",
    "y = (X > 0).astype(np.float).ravel()\n",
    "\n",
    "# Add some noise\n",
    "X[X > 0] *= 4  # strectch out values over 0\n",
    "\n",
    "# Be sure to find samples under 0 with y=1 and vice versa\n",
    "X += .3 * np.random.normal(size=n_samples)  \n",
    "\n",
    "# Finally, make it a col vector\n",
    "X = X[:, np.newaxis]\n",
    "\n",
    "# Plot the point distribution\n",
    "plt.figure(figsize=(17, 5))\n",
    "plt.scatter(X, y, color='black')\n",
    "\n",
    "# Instantiate the classifier\n",
    "clf = linear_model.LogisticRegression(C=1e7, solver='lbfgs')\n",
    "clf.fit(X, y)\n",
    "w0, w1 = clf.intercept_, clf.coef_\n",
    "\n",
    "def log(x):\n",
    "    \"\"\"Get the probability using a logistic function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Create a bunch of test samples and predict the values and the odds for them.\n",
    "X_test = np.linspace(-1, 2, n_samples)\n",
    "y_hat = clf.predict(X_test[:, np.newaxis])\n",
    "odds = log(w1 * X_test + w0).ravel()\n",
    "\n",
    "# Plot the outcomes\n",
    "plt.plot(X_test, odds, color='red', linewidth=3, label='Probability')\n",
    "plt.plot(X_test, y_hat, color='blue', linewidth=3, label='Prediction (y_hat)')\n",
    "\n",
    "# Finally, add some more details to the plot\n",
    "plt.axhline(.5, color='.5') # 2:2 odds\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('X')\n",
    "plt.xlim(-1, 2)\n",
    "plt.legend( loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Example #2, logistic regresion with iris dataset**\n",
    "An adaptation from a [[scikit-learn]](https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html#sphx-glr-auto-examples-linear-model-plot-iris-logistic-py) example. \n",
    "\n",
    "In this example, we'll tray to set a linear boundary between the points so blue ones are in blue areas, yellows in yellow and browns in brown.\n",
    "\n",
    "We can imagine this boundaries as different heights (Z). We can see that blue ones are quite accurate classified whereas the main problem is addressed at brown ones.\n",
    "\n",
    "The second graph shows how some pair of features are more suitable to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, 2:4]  # we only take the first two features.\n",
    "Y = iris.target\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg = linear_model.LogisticRegression(\n",
    "    C=1, solver='lbfgs', multi_class='multinomial')\n",
    "logreg.fit(X, Y)\n",
    "\n",
    "# Plot the decision boundary.\n",
    "xx, yy = mesh(X)\n",
    "\n",
    "# Drop all the points into the model and predict the values for them\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(7,7))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Multi-comparison between pairs of features.\n",
    "\n",
    "# Let's take features by couples to compare them\n",
    "X = (IRIS.data[:, :2], IRIS.data[:, 1:3], IRIS.data[:, 2:4])\n",
    "Y = IRIS.target\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg = linear_model.LogisticRegression(\n",
    "    C=1e5, solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "# Instantiate the plot\n",
    "_, axs = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "for n, pair in enumerate(X):\n",
    "    # Fit the model\n",
    "    clf = logreg.fit(pair, Y)\n",
    "    \n",
    "    # Get the boundaries\n",
    "    xx, yy = mesh(pair)\n",
    "    \n",
    "    # Make a bunch of predictions\n",
    "    y_hat = (clf.predict(np.c_[xx.ravel(), yy.ravel()]))\n",
    "    Z = (y_hat.reshape(xx.shape))\n",
    "    \n",
    "    # Plot the classification boundaries\n",
    "    axs[n].pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    \n",
    "    # And the training points\n",
    "    axs[n].scatter(pair[:, 0], pair[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "    \n",
    "    # Now set the limits, labels and remove the ticks\n",
    "    axs[n].set_xlim(xx.min(), xx.max())\n",
    "    axs[n].set_ylim(yy.min(), yy.max())\n",
    "    axs[n].set_xlabel(IRIS.feature_names[n])\n",
    "    axs[n].set_ylabel(IRIS.feature_names[n + 1])\n",
    "    axs[n].set_xticks(()), axs[n].set_yticks(())\n",
    "    \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminant analysis\n",
    "\n",
    "The linear and quadratic discriminant analysis is an algorithm that can be used for classification and also for dimensionality reduction (especially when dealing with multiclass datasets)\n",
    "\n",
    "It's based on Bayesian inference:\n",
    "\n",
    "$$P(y=k|\\mathbf{X})=\\frac{P(\\mathbf{X}|y=k)\\cdot P(y=k)}{P(\\mathbf{X})}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $P(y=k|\\mathbf{X})$, **estimation:** probability that the known vector $\\mathbf{X}$ (test dataset vector) belongs to certain class $k$\n",
    "\n",
    "* $P(\\mathbf{X}|y=k)$, **likelihood:** take all the vectors $\\mathbf{X}$ in the training dataset that output certain class $k$ and assume that each component (feature) is normal distributed.\n",
    "\n",
    "* $P(y=k)$, **priors:** ratio of class $k$ in the training dataset.\n",
    "\n",
    "* $P(\\mathbf{X})$: an element we can get rid of since the dataset is fixed and therefore its probability is 1.\n",
    "\n",
    "With all this above we get:\n",
    "\n",
    "> **Estimation(k) = likelihood(k) · priors(k)** and then,\n",
    ">\n",
    "> `max[estimation(k) for k in classes]`\n",
    "\n",
    "That is, we'll calculate all the estimations for each possible class for a given vector in the test dataset and choose the $k$ that has max value (maximum likelihood\n",
    "\n",
    "LDA assumes that each class has the same covariance matrix (that is distributions have similar eigenvectors but different means) and that implies that the classification boundary is a straight line. Intuitivelly, there's a line that shows the equilibrium between probabilities, everyone is pulling vector **X** towards itself with the same strenght.\n",
    "\n",
    "Alternativelly, QDA makes no assumptions and that leads to quadratic boundaries, we are squaring the distance from vector **X** to the mean of each class.\n",
    "\n",
    "[[Scikit-learn]](https://scikit-learn.org/stable/modules/lda_qda.html#mathematical-formulation-of-the-lda-and-qda-classifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# First, generate datasets\n",
    "def dataset_fixed_cov(n=300):\n",
    "    \"\"\"Generate 2 Gaussian samples with the same covariance matrix\"\"\"\n",
    "    dim = 2\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # generate two random samples \n",
    "    s1, s2 = np.random.randn(n, dim), np.random.randn(n, dim)\n",
    "    \n",
    "    # define a linear transformation\n",
    "    T = np.array([[0., -0.23], [0.83, .23]])\n",
    "    \n",
    "    # Apply transformations and ensure s2 is far enough\n",
    "    s1, s2 = np.dot(s1, T), np.dot(s2, T) + np.array([1, 1])\n",
    "    \n",
    "    # Finally join them together\n",
    "    X = np.r_[s1, s2]\n",
    "    \n",
    "    # Assign classes\n",
    "    y = np.hstack((np.zeros(n), np.ones(n)))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def dataset_cov(n=300):\n",
    "    \"\"\"Generate 2 Gaussian samples with different covariance matrices.\"\"\"\n",
    "    dim = 2\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # generate two random samples \n",
    "    s1, s2 = np.random.randn(n, dim), np.random.randn(n, dim)\n",
    "    \n",
    "    # define a linear transformation\n",
    "    T = np.array([[0., -1.], [2.5, .7]]) * 2\n",
    "    \n",
    "    # Apply transformations and ensure s2 is far enough\n",
    "    s1, s2 = np.dot(s1, T), np.dot(s2, T.T) + np.array([1, 4])\n",
    "    \n",
    "    # Finally join them together\n",
    "    X = np.r_[s1, s2]\n",
    "    \n",
    "    # Create the classes\n",
    "    y = np.hstack((np.zeros(n), np.ones(n)))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# #############################################################################\n",
    "# Second, generate a colormap.\n",
    "cmap = LinearSegmentedColormap(\n",
    "    'red_blue_classes',\n",
    "    {'red': [(0, 1, 1), (1, 0.7, 0.7)],\n",
    "     'green': [(0, 0.7, 0.7), (1, 0.7, 0.7)],\n",
    "     'blue': [(0, 0.7, 0.7), (1, 1, 1)]})\n",
    "plt.cm.register_cmap(cmap=cmap)\n",
    "\n",
    "# #############################################################################\n",
    "# Third, plot functions\n",
    "def plot_data(lda, X_train, X_test, y_test, y_hat, fig_index):\n",
    "    \"\"\"Plot the data for each of the subplots.\"\"\"\n",
    "    # Instatiate subplot to assign it the properties \n",
    "    splot = plt.subplot(2, 2, fig_index)\n",
    "    \n",
    "    # Add some titles\n",
    "    if fig_index == 1:\n",
    "        plt.title('Linear Discriminant Analysis')\n",
    "        plt.ylabel('Data with\\n fixed covariance')\n",
    "    elif fig_index == 2:\n",
    "        plt.title('Quadratic Discriminant Analysis')\n",
    "    elif fig_index == 3:\n",
    "        plt.ylabel('Data with\\n varying covariances')\n",
    "\n",
    "    ### # Generate a confusion matrix # ###\n",
    "    tp = (y_test == y_hat)  # Define what means true positive\n",
    "    tp0, tp1 = tp[y_test == 0], tp[y_test == 1]  # Group truths by class\n",
    "    X0, X1 = X_test[y_test == 0], X_test[y_test == 1]  # Group samples by class also\n",
    "    X0_tp, X0_fp = X0[tp0], X0[~tp0]  # Select tp & fp in class 0\n",
    "    X1_tp, X1_fp = X1[tp1], X1[~tp1]  # Select tp & fp in class 1\n",
    "\n",
    "    # plot points \n",
    "    # dots are tp and crosses are fp, red are class 0 and blue are class 1\n",
    "    plt.scatter(X0_tp[:, 0], X0_tp[:, 1], marker='.', color='red')\n",
    "    plt.scatter(X0_fp[:, 0], X0_fp[:, 1], marker='x', s=20, color='#990000')\n",
    "    plt.scatter(X1_tp[:, 0], X1_tp[:, 1], marker='.', color='blue')\n",
    "    plt.scatter(X1_fp[:, 0], X1_fp[:, 1], marker='x', s=20, color='#000099')  # dark blue\n",
    "\n",
    "    # Get a bunch of points inside the values of X\n",
    "    xx, yy = mesh(X_train, h=.1)\n",
    "    \n",
    "    # Drop'em all into the model and predict the values for them\n",
    "    Z = lda.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Plot the outcome into a color plot\n",
    "    Z = Z[:, 1].reshape(xx.shape)\n",
    "    plt.pcolormesh(xx, yy, Z, cmap='red_blue_classes',\n",
    "                   norm=Normalize(0., 1.), zorder=0)\n",
    "    \n",
    "    # Also paint the border line\n",
    "    plt.contour(xx, yy, Z, [0.5], linewidths=2., colors='white')\n",
    "    \n",
    "    # Add some limits to the graph\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "    # Finally draw a star where the means are located\n",
    "    plt.plot(lda.means_[0][0], lda.means_[0][1],\n",
    "             '*', color='yellow', markersize=15, markeredgecolor='grey')\n",
    "    plt.plot(lda.means_[1][0], lda.means_[1][1],\n",
    "             '*', color='yellow', markersize=15, markeredgecolor='grey')\n",
    "\n",
    "    return splot\n",
    "\n",
    "\n",
    "def plot_ellipse(splot, mean, cov, color):\n",
    "    \"\"\"Draw an ellipse to show standard deviation.\"\"\"\n",
    "    v, w = np.linalg.eigh(cov)  # Get eigenvalues & eigenvectors\n",
    "    u = w[0] / np.linalg.norm(w[0])  # Get the unit eigenvector\n",
    "    angle = np.arctan(u[1] / u[0])  # Get the angle, in radians\n",
    "    angle = 180 * angle / np.pi  # convert to degrees\n",
    "    \n",
    "    # filled Gaussian at 2 standard deviation\n",
    "    ell = Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,\n",
    "                              180 + angle, facecolor=color,\n",
    "                              edgecolor='black', linewidth=2)\n",
    "    \n",
    "    # Add some extra+ art\n",
    "    ell.set_clip_box(splot.bbox)\n",
    "    ell.set_alpha(0.2)\n",
    "    splot.add_artist(ell)\n",
    "    splot.set_xticks(()), splot.set_yticks(())\n",
    "\n",
    "\n",
    "def plot_lda_cov(lda, splot):\n",
    "    \"\"\"Invoke the ellipses.\"\"\"\n",
    "    plot_ellipse(splot, lda.means_[0], lda.covariance_, 'red')\n",
    "    plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')\n",
    "\n",
    "\n",
    "def plot_qda_cov(qda, splot):\n",
    "    \"\"\"Invoke the ellipses.\"\"\"\n",
    "    plot_ellipse(splot, qda.means_[0], qda.covariance_[0], 'red')\n",
    "    plot_ellipse(splot, qda.means_[1], qda.covariance_[1], 'blue')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(17, 12), facecolor='white')\n",
    "plt.suptitle('Linear Discriminant Analysis vs Quadratic Discriminant Analysis',\n",
    "             y=0.98, fontsize=15)\n",
    "\n",
    "# #############################################################################\n",
    "# Fourth, generate the plots\n",
    "\n",
    "# Get confusion matrices\n",
    "cml, cmq = list(), list()\n",
    "\n",
    "for i, (X, y) in enumerate([dataset_fixed_cov(n=600), dataset_cov(n=600)]):\n",
    "    # Split the datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y)\n",
    "    \n",
    "    # Instatiate the estimator\n",
    "    lda = LinearDiscriminantAnalysis(solver=\"lsqr\", store_covariance=True)\n",
    "    \n",
    "    # Make some predictions on the test set\n",
    "    y_hat = lda.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    # Get the confusion matrix for them\n",
    "    cml.append(confusion_matrix(y_test, y_hat))\n",
    "    \n",
    "    # Finally plot the outcome\n",
    "    splot = plot_data(lda, X_train, X_test, y_test, y_hat, fig_index=2 * i + 1)\n",
    "    plot_lda_cov(lda, splot)\n",
    "    plt.axis('tight')\n",
    "\n",
    "    # Quadratic Discriminant Analysis\n",
    "    # Instantiate the estimator\n",
    "    qda = QuadraticDiscriminantAnalysis(store_covariance=True)\n",
    "    \n",
    "    # Make some predictions using same set\n",
    "    y_hat = qda.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    # Get the confusion matrix for them\n",
    "    cmq.append(confusion_matrix(y_test, y_hat))\n",
    "    \n",
    "    # Finally plot the outcome\n",
    "    splot = plot_data(qda, X_train, X_test, y_test, y_hat, fig_index=2 * i + 2)\n",
    "    plot_qda_cov(qda, splot)\n",
    "    plt.axis('tight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.92)\n",
    "plt.show()\n",
    "\n",
    "# Print the outcomes of confusion matrix\n",
    "data = {\n",
    "    'Value': ['TP', 'FP', 'FN', 'TN', ],\n",
    "    'LDA fixed': cml[0].ravel(),\n",
    "    'LDA var': cml[1].ravel(),\n",
    "    'QDA fixed': cmq[0].ravel(),\n",
    "    'QDA var': cmq[1].ravel(),\n",
    "}\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "**Bayes theorem:**  \n",
    "Bayes theorem let us update our beliefs (priors, $y$ classes) when we observe new evidence ($\\mathbf{X}$ training).\n",
    "\n",
    "**Naive Bayes:**\n",
    "> The probability of a future class ($\\hat{y}$) given certain feature ($\\mathbf{X}_i$) is equal to the probability --distribution-- of that feature in the training times the ratio of that class in the  whole training set.\n",
    ">\n",
    "> Then we can choose the $\\hat{y}$ that gives the highest value (most likely)\n",
    "\n",
    "Naive since it assumes that  features are independent one each other and therefore the covariance matrix is diagonal (all the values are 0 except each component with himself)\n",
    "\n",
    "**Algorithms**  \n",
    "There are 4 algorithms to work out naive Bayes:\n",
    "* Gaussian: the likelihood of the feature is assumed to be Gaussian\n",
    "* Multinomial: the likelihood of each feature is assumed to be Multinomial (n-binomial)\n",
    "* Complement: the above multinomial for unbalanced classes.\n",
    "* Bernoulli: the likelihood of the feature is assumed to be a multivariate Bernoulli dostribution.\n",
    "\n",
    "One of the most common uses of Naive Bayes is for text classification (like spam filtering for email) where one can assume that the words in the message are independent events. This condition is not generally satisfied (for example, in natural languages like English the probability of finding an adjective is affected by the probability of having a noun), but it is a useful idealization, especially since the statistical correlations between individual words are usually not known.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X, y = IRIS.data, IRIS.target\n",
    "\n",
    "# Instantiate the classifier\n",
    "gnb = GaussianNB()\n",
    "clf = gnb.fit(X, y)\n",
    "\n",
    "# Make some predictions\n",
    "y_hat = clf.predict(X)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X.shape[0],(y != y_hat).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points.\n",
    "\n",
    "The closest points to the plane are the so called *support vectors* since they support the plane\n",
    "\n",
    "Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane\n",
    "\n",
    "SVC is a **linear classifier** that can be converted to non-linear through the so-called *kernel trick*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remove one of the classes\n",
    "svc_df = IRIS_PD.drop((IRIS_PD[IRIS_PD['target'] == 2]).index)\n",
    "x, y= 'sepal length (cm)', 'petal length (cm)'\n",
    "svc_df.plot.scatter(x=x, y=y)\n",
    "\n",
    "# Replace target 0 by -1 so sign can change with the product afterwards\n",
    "t1 = svc_df[svc_df['target'] == 0]\n",
    "svc_df.loc[t1.index, 'target'] = -1\n",
    "\n",
    "# Get vectors and targets\n",
    "X = svc_df[['sepal length (cm)', 'petal length (cm)']].values \n",
    "Y = svc_df['target'].values \n",
    "\n",
    "# Shuffle a bit and split into train and test\n",
    "X, Y = shuffle(X, Y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.9)\n",
    "\n",
    "# Make targets col vectors\n",
    "y_train, y_test = y_train.reshape(90, 1), y_test.reshape(10, 1)\n",
    "\n",
    "# Also for features\n",
    "train_f1 = x_train[:, 0].reshape(90, 1)\n",
    "train_f2 = x_train[:, 1].reshape(90, 1)\n",
    "\n",
    "# Initialize the coefficients\n",
    "w1, w2 = 0, 0\n",
    "\n",
    "# Training cycles & learning rate\n",
    "epochs, alpha = 1000, 1e-4\n",
    "\n",
    "# record the evolution of w\n",
    "w1_tape, w2_tape = np.zeros(0), np.zeros(0)\n",
    "\n",
    "# Now, start training\n",
    "for e in range(1, epochs):\n",
    "    \n",
    "    \"\"\"Let's calculate the dot product between the 90 instances of x (with two features) \n",
    "    and 90 instances of w (with also two features)\"\"\"\n",
    "    y_hat = w1 * train_f1 + w2 * train_f2\n",
    "    \n",
    "    # And change the sign of those instances that are in one category\n",
    "    prod = y_hat * y_train\n",
    "    \n",
    "    # regularization parameter (reduces the impact of grad w over training)\n",
    "    # Used to produce stable solutions\n",
    "    lamda = 1 / (epochs)\n",
    "    \n",
    "    \n",
    "    for n, val in enumerate(prod):\n",
    "        grad_w1, grad_w2 = 2 * lamda * w1, 2 * lamda * w2\n",
    "        if val >= 1:\n",
    "            w1 -= alpha * grad_w1\n",
    "            w2 -= alpha * grad_w2\n",
    "        else:\n",
    "            loss_f1 = train_f1[n] * y_train[n]\n",
    "            loss_f2 = train_f2[n] * y_train[n]\n",
    "            w1 += alpha * (loss_f1 -  grad_w1)\n",
    "            w2 += alpha * (loss_f2 -  grad_w2)\n",
    "\n",
    "    w1_tape = np.append(w1_tape, w1[0])\n",
    "    w2_tape = np.append(w2_tape, w2[0])\n",
    "    \n",
    "    print('Epoch: {}\\r'.format(e), end='')\n",
    "    \n",
    "### Plot ###\n",
    "x = np.arange(1, epochs)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "p1 = ax.plot(x, w1_tape)\n",
    "p2 = ax.plot(x, w2_tape)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Get test features\n",
    "test_f1 = x_test[:, 0].reshape(10, 1)\n",
    "test_f2 = x_test[:, 1].reshape(10, 1)\n",
    "\n",
    "# Predict'em\n",
    "y_hat = w1 * test_f1 + w2 * test_f2\n",
    "\n",
    "pred = [1 if val>1 else -1 for val in y_hat]\n",
    "print('precission:', accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a maximum margin separating hyperplane\n",
    "[sklearn origin](https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 40 separable points\n",
    "X, y = datasets.make_blobs(n_samples=200, centers=2, random_state=6)\n",
    "\n",
    "# Fit the model\n",
    "clf = SVC(kernel='linear', C=1000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot the decision function\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "\n",
    "# plot support vectors\n",
    "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none', edgecolors='k')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA for classification\n",
    "\n",
    "[[PCA visualized]](https://notsquirrel.com/pca/)  \n",
    "[[SVD insight]](https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/)\n",
    "\n",
    "Principal component analysis (PCA), is a techique that let's us to get a reduced & efficient version of a given matrix (covariance matrix, *sometimes, often, always?*), that can be used in turn for predictions. Efficient since provides eigenvectors (unique information) and reduced since only a few of these eigenvectors are enough to give close approximations to the original matrix. \n",
    "\n",
    "**Process breakdown (PCA visualized):**\n",
    "\n",
    "1. Flatten every image to an array of $1\\times 64$\n",
    "2. Get the expected value of every pixel across all flattened images\n",
    "3. Get the expected value over the N images of the distance from each pixel to the pixel mean\n",
    "4. Build the covariance matrix, that is a $64\\times 64$ matrix of distances and relationships.  \n",
    "    If some pixel is:\n",
    "    * Large positive, then if $i>\\mu$,  $j>\\mu$\n",
    "    * Large negative, then if $i>\\mu$, $j<\\mu$\n",
    "    * Close to zero, then $i$ doest not provide much information about $j$\n",
    "5. The goal of this process is to get an efficient version of above created matrix so it can be used to predict unknown images. This means that we should find the eigenvectors of this matrix, and this is achieved by descompositing it with [SVD](../Glossary.ipynb/#S) (Singular value descomposition). These eigenvectors we are looking for are the columns in $\\mathbf{U}$\n",
    "6. Apply the reduced matrix over an unknown image, if the outcome is the image ehanced then it agrees with the training otherwise returns noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Dimensionality reduction with SVD\n",
    "# Forked from: http://cs231n.github.io/neural-networks-2/\n",
    "\n",
    "# Create a sample matrix\n",
    "X = np.random.normal(size=(100, 10))\n",
    "\n",
    "# Get the covariance matrix\n",
    "X -= X.mean(axis=0)  # Center at zero\n",
    "cov = np.dot(X.T, X) / X.shape[0]\n",
    "\n",
    "# Decompose it using SVD\n",
    "U, s, V_T = np.linalg.svd(cov)\n",
    "\n",
    "# Decorrelate the data\n",
    "Xrot = np.dot(X, U)\n",
    "\n",
    "# Reduce dimensions\n",
    "Xrot_reduced = np.dot(X, U[:, :4])\n",
    "\n",
    "Xrot_reduced.shape  # 100x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
